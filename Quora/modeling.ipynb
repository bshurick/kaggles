{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quora\n",
    "\n",
    "https://www.kaggle.com/c/quora-insincere-questions-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.1) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Native\n",
    "import os\n",
    "import string\n",
    "import timeit\n",
    "import gc\n",
    "import re\n",
    "\n",
    "# Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import gensim\n",
    "\n",
    "# modeling\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "\n",
    "# RNN\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, GRU, Dense, Bidirectional, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "import math\n",
    "\n",
    "\n",
    "# XGB\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# cython\n",
    "import Cython\n",
    "%load_ext Cython\n",
    "\n",
    "# Viz\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASEDIR = '/Users/brandonshurick/Kaggles/quora-insincere-questions-classification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(BASEDIR, 'train.csv.zip'))\n",
    "test = pd.read_csv(os.path.join(BASEDIR, 'test.csv.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you enco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>Can I convert montra helicon D to a mountain b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                                      question_text  \\\n",
       "0  00002165364db923c7e6  How did Quebec nationalists see their province...   \n",
       "1  000032939017120e6e44  Do you have an adopted dog, how would you enco...   \n",
       "2  0000412ca6e4628ce2cf  Why does velocity affect time? Does velocity a...   \n",
       "3  000042bf85aa498cd78e  How did Otto von Guericke used the Magdeburg h...   \n",
       "4  0000455dfa3e01eae3af  Can I convert montra helicon D to a mountain b...   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "vecmodel = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    os.path.join(\n",
    "        BASEDIR,\n",
    "        'GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\n",
    "    ), binary=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [02:12, 16536.49it/s]\n",
      "1703756it [01:44, 16381.83it/s]\n",
      "999995it [00:57, 17288.25it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "embeddings_index = {}\n",
    "embeddings_index2 = {}\n",
    "embeddings_index3 = {}\n",
    "\n",
    "f = open(os.path.join(BASEDIR,'glove.840B.300d/glove.840B.300d.txt'))\n",
    "f2 = open(os.path.join(BASEDIR,'paragram_300_sl999/paragram_300_sl999.txt'), encoding='latin') \n",
    "f3 = open(os.path.join(BASEDIR,'wiki-news-300d-1M/wiki-news-300d-1M.vec'), encoding='latin')\n",
    "\n",
    "# # glove\n",
    "for line in tqdm(f):\n",
    "    values = line.split(\" \")\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "\n",
    "# paragram\n",
    "for line in tqdm(f2):\n",
    "    values = line.split(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index2[word] = coefs\n",
    "        \n",
    "# wikinews\n",
    "for line in tqdm(f3):\n",
    "    values = line.split(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    if len(coefs) == 300:\n",
    "        embeddings_index3[word] = coefs\n",
    "\n",
    "f.close()\n",
    "f2.close()\n",
    "f3.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate all embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.DataFrame(embeddings_index).T\n",
    "x2 = pd.DataFrame(embeddings_index2).T\n",
    "x3 = pd.DataFrame(embeddings_index3).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2.columns = np.arange(300,600)\n",
    "x3.columns = np.arange(600,900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = pd.concat((x,x2,x3),axis=1,sort=False).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3144144, 900)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = dict(zip(embeddings.index, embeddings.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x; del x2; del x3\n",
    "del embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom tokenize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def tokenize(text):\n",
    "    \n",
    "    # Split up words\n",
    "    trans_table = {ord(c): None for c in string.punctuation + string.digits}  \n",
    "    tokens = [\n",
    "        word \n",
    "        for word in nltk.word_tokenize(text.translate(trans_table)) \n",
    "        if len(word) > 1\n",
    "        and word not in stop_words\n",
    "    ]\n",
    "    \n",
    "    # Stem\n",
    "    stems = [ stemmer.stem(item) for item in tokens if item not in stop_words ]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thi', 'test', 'sentenc', 'horribl', 'crap']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize('This is only a test sentence and it is horrible crap.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TDIDF matrix"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tf = TfidfVectorizer(\n",
    "    max_features=10000, \n",
    "    ngram_range=(1,3), \n",
    "    tokenizer=tokenize,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "start_time = timeit.default_timer()\n",
    "_ = tf.fit_transform( train.question_text.values )\n",
    "elapsed = timeit.default_timer() - start_time"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print('Runtime: {:0.2f} minutes'.format(elapsed / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked modeling with kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "cimport numpy as np\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# embedding first 30 words \n",
    "cpdef np.ndarray text_to_array(str text, dict embeddings_index):\n",
    "    cdef np.ndarray empyt_emb\n",
    "    cdef list text_block\n",
    "    empyt_emb = np.zeros(900)\n",
    "    text_block = re.sub('([.,!?()\\'\"])', r' \\1', text).split()[:30]\n",
    "    embeds = [ embeddings_index.get(x, empyt_emb) for x in text_block ]\n",
    "    embeds+= [empyt_emb] * (30 - len(embeds))\n",
    "    return np.array(embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2**6\n",
    "def batch_gen(X, y):\n",
    "    n_batches = math.ceil(len(X) / batch_size)\n",
    "    while True: \n",
    "        X = X.sample(frac=1.)  # Shuffle the data.\n",
    "        y = y[X.index]\n",
    "        for i in range(n_batches):\n",
    "            texts = X.iloc[i*batch_size:(i+1)*batch_size]\n",
    "            text_arr = np.array([text_to_array(text, embeddings_index) for text in texts])\n",
    "            yield text_arr, np.array(y[i*batch_size:(i+1)*batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running fold 1\n",
      "\n",
      "\tConvert word vectors to set of arrays\n",
      "\tModel 6, RNN\n",
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 384s 384ms/step - loss: 0.1328 - acc: 0.9502 - val_loss: 0.1233 - val_acc: 0.9537\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 325s 325ms/step - loss: 0.1185 - acc: 0.9528 - val_loss: 0.1119 - val_acc: 0.9563\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 312s 312ms/step - loss: 0.1128 - acc: 0.9547 - val_loss: 0.1084 - val_acc: 0.9573\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 310s 310ms/step - loss: 0.1097 - acc: 0.9565 - val_loss: 0.1090 - val_acc: 0.9572\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 308s 308ms/step - loss: 0.1093 - acc: 0.9559 - val_loss: 0.1090 - val_acc: 0.9565\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 302s 302ms/step - loss: 0.1085 - acc: 0.9566 - val_loss: 0.1069 - val_acc: 0.9581\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 299s 299ms/step - loss: 0.1073 - acc: 0.9577 - val_loss: 0.1073 - val_acc: 0.9570\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 295s 295ms/step - loss: 0.1074 - acc: 0.9574 - val_loss: 0.1072 - val_acc: 0.9572\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 294s 294ms/step - loss: 0.1048 - acc: 0.9583 - val_loss: 0.1083 - val_acc: 0.9566\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 306s 306ms/step - loss: 0.1069 - acc: 0.9574 - val_loss: 0.1034 - val_acc: 0.9591\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 301s 301ms/step - loss: 0.1090 - acc: 0.9563 - val_loss: 0.1035 - val_acc: 0.9597\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 297s 297ms/step - loss: 0.1053 - acc: 0.9580 - val_loss: 0.1029 - val_acc: 0.9593\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 303s 303ms/step - loss: 0.1011 - acc: 0.9603 - val_loss: 0.1033 - val_acc: 0.9601\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 297s 297ms/step - loss: 0.1058 - acc: 0.9584 - val_loss: 0.1020 - val_acc: 0.9596\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 294s 294ms/step - loss: 0.1014 - acc: 0.9595 - val_loss: 0.1022 - val_acc: 0.9593\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 301s 301ms/step - loss: 0.1023 - acc: 0.9599 - val_loss: 0.1032 - val_acc: 0.9587\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 293s 293ms/step - loss: 0.1021 - acc: 0.9593 - val_loss: 0.1015 - val_acc: 0.9596\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 297s 297ms/step - loss: 0.1015 - acc: 0.9600 - val_loss: 0.1009 - val_acc: 0.9602\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 290s 290ms/step - loss: 0.0970 - acc: 0.9602 - val_loss: 0.1012 - val_acc: 0.9602\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 290s 290ms/step - loss: 0.0966 - acc: 0.9611 - val_loss: 0.1019 - val_acc: 0.9596\n",
      "TFIDF transform\n",
      "\tModel 1, Naive Bayes\n",
      "\tModel 2, Huber\n",
      "\tModel 3, SVM\n",
      "\tModel 4, Logistic Regression\n",
      "\tModel 5, LGB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/lightgbm/engine.py:116: UserWarning: Found `num_trees` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\teval's binary_logloss: 0.137765\n",
      "[200]\teval's binary_logloss: 0.131845\n",
      "[300]\teval's binary_logloss: 0.129138\n",
      "[400]\teval's binary_logloss: 0.127661\n",
      "[500]\teval's binary_logloss: 0.126708\n",
      "[600]\teval's binary_logloss: 0.126074\n",
      "[700]\teval's binary_logloss: 0.125628\n",
      "[800]\teval's binary_logloss: 0.125213\n",
      "[900]\teval's binary_logloss: 0.124961\n",
      "[1000]\teval's binary_logloss: 0.124781\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[999]\teval's binary_logloss: 0.12478\n",
      "\n",
      "Running fold 2\n",
      "\n",
      "\tConvert word vectors to set of arrays\n",
      "\tModel 6, RNN\n",
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 360s 360ms/step - loss: 0.1359 - acc: 0.9493 - val_loss: 0.1194 - val_acc: 0.9529\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 300s 300ms/step - loss: 0.1173 - acc: 0.9534 - val_loss: 0.1143 - val_acc: 0.9542\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 293s 293ms/step - loss: 0.1144 - acc: 0.9557 - val_loss: 0.1137 - val_acc: 0.9548\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 291s 291ms/step - loss: 0.1070 - acc: 0.9577 - val_loss: 0.1117 - val_acc: 0.9556\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 299s 299ms/step - loss: 0.1092 - acc: 0.9565 - val_loss: 0.1105 - val_acc: 0.9557\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 300s 300ms/step - loss: 0.1098 - acc: 0.9567 - val_loss: 0.1107 - val_acc: 0.9559\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 297s 297ms/step - loss: 0.1058 - acc: 0.9592 - val_loss: 0.1107 - val_acc: 0.9559\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 301s 301ms/step - loss: 0.1080 - acc: 0.9561 - val_loss: 0.1057 - val_acc: 0.9581\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 302s 302ms/step - loss: 0.1042 - acc: 0.9578 - val_loss: 0.1079 - val_acc: 0.9572\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 300s 300ms/step - loss: 0.1059 - acc: 0.9585 - val_loss: 0.1052 - val_acc: 0.9581\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 296s 296ms/step - loss: 0.1069 - acc: 0.9586 - val_loss: 0.1048 - val_acc: 0.9585\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 302s 302ms/step - loss: 0.1024 - acc: 0.9594 - val_loss: 0.1045 - val_acc: 0.9581\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 307s 307ms/step - loss: 0.0998 - acc: 0.9593 - val_loss: 0.1048 - val_acc: 0.9583\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 300s 300ms/step - loss: 0.1047 - acc: 0.9585 - val_loss: 0.1038 - val_acc: 0.9588\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 303s 303ms/step - loss: 0.1038 - acc: 0.9591 - val_loss: 0.1044 - val_acc: 0.9585\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 295s 295ms/step - loss: 0.1010 - acc: 0.9592 - val_loss: 0.1043 - val_acc: 0.9584\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 294s 294ms/step - loss: 0.1025 - acc: 0.9589 - val_loss: 0.1033 - val_acc: 0.9590\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 296s 296ms/step - loss: 0.1046 - acc: 0.9585 - val_loss: 0.1029 - val_acc: 0.9593\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 310s 310ms/step - loss: 0.0976 - acc: 0.9610 - val_loss: 0.1035 - val_acc: 0.9592\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 301s 301ms/step - loss: 0.0952 - acc: 0.9615 - val_loss: 0.1030 - val_acc: 0.9589\n",
      "TFIDF transform\n",
      "\tModel 1, Naive Bayes\n",
      "\tModel 2, Huber\n",
      "\tModel 3, SVM\n",
      "\tModel 4, Logistic Regression\n",
      "\tModel 5, LGB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/lightgbm/engine.py:116: UserWarning: Found `num_trees` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\teval's binary_logloss: 0.140893\n",
      "[200]\teval's binary_logloss: 0.134888\n",
      "[300]\teval's binary_logloss: 0.13217\n",
      "[400]\teval's binary_logloss: 0.130659\n",
      "[500]\teval's binary_logloss: 0.12971\n",
      "[600]\teval's binary_logloss: 0.129081\n",
      "[700]\teval's binary_logloss: 0.128599\n",
      "[800]\teval's binary_logloss: 0.128179\n",
      "[900]\teval's binary_logloss: 0.127988\n",
      "[1000]\teval's binary_logloss: 0.127815\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[999]\teval's binary_logloss: 0.127815\n",
      "\n",
      "Running fold 3\n",
      "\n",
      "\tConvert word vectors to set of arrays\n",
      "\tModel 6, RNN\n",
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 329s 329ms/step - loss: 0.1337 - acc: 0.9496 - val_loss: 0.1218 - val_acc: 0.9515\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 295s 295ms/step - loss: 0.1196 - acc: 0.9519 - val_loss: 0.1127 - val_acc: 0.9553\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 294s 294ms/step - loss: 0.1135 - acc: 0.9558 - val_loss: 0.1105 - val_acc: 0.9563\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 295s 295ms/step - loss: 0.1108 - acc: 0.9560 - val_loss: 0.1111 - val_acc: 0.9561\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 294s 294ms/step - loss: 0.1103 - acc: 0.9565 - val_loss: 0.1087 - val_acc: 0.9573\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 296s 296ms/step - loss: 0.1087 - acc: 0.9572 - val_loss: 0.1074 - val_acc: 0.9578\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 292s 292ms/step - loss: 0.1068 - acc: 0.9577 - val_loss: 0.1059 - val_acc: 0.9582\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 291s 291ms/step - loss: 0.1031 - acc: 0.9594 - val_loss: 0.1052 - val_acc: 0.9586\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 296s 296ms/step - loss: 0.1045 - acc: 0.9577 - val_loss: 0.1055 - val_acc: 0.9582\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 287s 287ms/step - loss: 0.1049 - acc: 0.9589 - val_loss: 0.1046 - val_acc: 0.9583\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 289s 289ms/step - loss: 0.1032 - acc: 0.9583 - val_loss: 0.1042 - val_acc: 0.9583\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 296s 296ms/step - loss: 0.1036 - acc: 0.9583 - val_loss: 0.1039 - val_acc: 0.9587\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 292s 292ms/step - loss: 0.1047 - acc: 0.9586 - val_loss: 0.1037 - val_acc: 0.9586\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 291s 291ms/step - loss: 0.1061 - acc: 0.9574 - val_loss: 0.1034 - val_acc: 0.9592\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 294s 294ms/step - loss: 0.1062 - acc: 0.9564 - val_loss: 0.1027 - val_acc: 0.9589\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 294s 294ms/step - loss: 0.1030 - acc: 0.9585 - val_loss: 0.1018 - val_acc: 0.9594\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 287s 287ms/step - loss: 0.1017 - acc: 0.9598 - val_loss: 0.1017 - val_acc: 0.9597\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 294s 294ms/step - loss: 0.0988 - acc: 0.9607 - val_loss: 0.1017 - val_acc: 0.9591\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 283s 283ms/step - loss: 0.0944 - acc: 0.9620 - val_loss: 0.1017 - val_acc: 0.9594\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 288s 288ms/step - loss: 0.0947 - acc: 0.9619 - val_loss: 0.1041 - val_acc: 0.9577\n",
      "TFIDF transform\n",
      "\tModel 1, Naive Bayes\n",
      "\tModel 2, Huber\n",
      "\tModel 3, SVM\n",
      "\tModel 4, Logistic Regression\n",
      "\tModel 5, LGB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/lightgbm/engine.py:116: UserWarning: Found `num_trees` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\teval's binary_logloss: 0.139349\n",
      "[200]\teval's binary_logloss: 0.133431\n",
      "[300]\teval's binary_logloss: 0.130684\n",
      "[400]\teval's binary_logloss: 0.129209\n",
      "[500]\teval's binary_logloss: 0.128287\n",
      "[600]\teval's binary_logloss: 0.127603\n",
      "[700]\teval's binary_logloss: 0.127113\n",
      "[800]\teval's binary_logloss: 0.126772\n",
      "[900]\teval's binary_logloss: 0.126454\n",
      "[1000]\teval's binary_logloss: 0.126283\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[998]\teval's binary_logloss: 0.126277\n",
      "\n",
      "Running fold 4\n",
      "\n",
      "\tConvert word vectors to set of arrays\n",
      "\tModel 6, RNN\n",
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 331s 331ms/step - loss: 0.1336 - acc: 0.9486 - val_loss: 0.1168 - val_acc: 0.9539\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 300s 300ms/step - loss: 0.1152 - acc: 0.9542 - val_loss: 0.1149 - val_acc: 0.9538\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 292s 292ms/step - loss: 0.1129 - acc: 0.9557 - val_loss: 0.1098 - val_acc: 0.9570\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 292s 292ms/step - loss: 0.1106 - acc: 0.9565 - val_loss: 0.1074 - val_acc: 0.9571\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 296s 296ms/step - loss: 0.1089 - acc: 0.9565 - val_loss: 0.1054 - val_acc: 0.9583\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 298s 298ms/step - loss: 0.1080 - acc: 0.9570 - val_loss: 0.1051 - val_acc: 0.9582\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 289s 289ms/step - loss: 0.1052 - acc: 0.9587 - val_loss: 0.1043 - val_acc: 0.9588\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 294s 294ms/step - loss: 0.1055 - acc: 0.9590 - val_loss: 0.1034 - val_acc: 0.9591\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 291s 291ms/step - loss: 0.1042 - acc: 0.9575 - val_loss: 0.1040 - val_acc: 0.9590\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 288s 288ms/step - loss: 0.1024 - acc: 0.9593 - val_loss: 0.1033 - val_acc: 0.9589\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 288s 288ms/step - loss: 0.1057 - acc: 0.9575 - val_loss: 0.1022 - val_acc: 0.9593\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 292s 292ms/step - loss: 0.1043 - acc: 0.9590 - val_loss: 0.1023 - val_acc: 0.9590\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 290s 290ms/step - loss: 0.1046 - acc: 0.9583 - val_loss: 0.1019 - val_acc: 0.9593\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 296s 296ms/step - loss: 0.1055 - acc: 0.9578 - val_loss: 0.1046 - val_acc: 0.9593\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 293s 293ms/step - loss: 0.1045 - acc: 0.9578 - val_loss: 0.1010 - val_acc: 0.9596\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 302s 302ms/step - loss: 0.1023 - acc: 0.9596 - val_loss: 0.1012 - val_acc: 0.9595\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 290s 290ms/step - loss: 0.1044 - acc: 0.9579 - val_loss: 0.1007 - val_acc: 0.9601\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 291s 291ms/step - loss: 0.1048 - acc: 0.9584 - val_loss: 0.1008 - val_acc: 0.9602\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 282s 282ms/step - loss: 0.0965 - acc: 0.9617 - val_loss: 0.1012 - val_acc: 0.9599\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 285s 285ms/step - loss: 0.0962 - acc: 0.9623 - val_loss: 0.1011 - val_acc: 0.9604\n",
      "TFIDF transform\n",
      "\tModel 1, Naive Bayes\n",
      "\tModel 2, Huber\n",
      "\tModel 3, SVM\n",
      "\tModel 4, Logistic Regression\n",
      "\tModel 5, LGB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/lightgbm/engine.py:116: UserWarning: Found `num_trees` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\teval's binary_logloss: 0.13686\n",
      "[200]\teval's binary_logloss: 0.131123\n",
      "[300]\teval's binary_logloss: 0.128528\n",
      "[400]\teval's binary_logloss: 0.127057\n",
      "[500]\teval's binary_logloss: 0.12616\n",
      "[600]\teval's binary_logloss: 0.125454\n",
      "[700]\teval's binary_logloss: 0.125009\n",
      "[800]\teval's binary_logloss: 0.124695\n",
      "[900]\teval's binary_logloss: 0.124377\n",
      "[1000]\teval's binary_logloss: 0.124132\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\teval's binary_logloss: 0.124132\n",
      "\n",
      "Running fold 5\n",
      "\n",
      "\tConvert word vectors to set of arrays\n",
      "\tModel 6, RNN\n",
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 334s 334ms/step - loss: 0.1365 - acc: 0.9486 - val_loss: 0.1227 - val_acc: 0.9515\n",
      "Epoch 2/20\n",
      " 999/1000 [============================>.] - ETA: 0s - loss: 0.1173 - acc: 0.9539"
     ]
    }
   ],
   "source": [
    "kfd = KFold(n_splits=8, random_state=22)\n",
    "\n",
    "\n",
    "\n",
    "class NN:\n",
    "    def __init__(self, layers=64, shape=(30, 300), model_type='GRU'):\n",
    "        \n",
    "        if model_type == 'LSTM':\n",
    "            \n",
    "            model = Sequential()\n",
    "            model.add(Bidirectional(LSTM(layers, return_sequences=True),\n",
    "                                    input_shape=shape))\n",
    "            model.add(Bidirectional(LSTM(layers)))\n",
    "            model.add(Dense(1, activation=\"sigmoid\"))\n",
    "            model.compile(loss='binary_crossentropy',\n",
    "                          optimizer='adam',\n",
    "                          metrics=['accuracy'])\n",
    "            self.model = model\n",
    "        \n",
    "        elif model_type == 'GRU':\n",
    "            \n",
    "            model = Sequential()\n",
    "            model.add(Bidirectional(GRU(layers, return_sequences=True),\n",
    "                                    input_shape=shape))\n",
    "            model.add(Bidirectional(GRU(layers)))\n",
    "            model.add(Dense(1, activation=\"sigmoid\"))\n",
    "            model.compile(loss='binary_crossentropy',\n",
    "                          optimizer='adam',\n",
    "                          metrics=['accuracy'])\n",
    "            self.model = model\n",
    "\n",
    "\n",
    "# XGB params\n",
    "param = {\n",
    "    'max_depth':4, \n",
    "    'eta':0.05, \n",
    "    'silent':1, \n",
    "    'objective':'binary:logistic' ,\n",
    "    'colsample_bytree': 0.85 ,\n",
    "    'subsample': 0.85 ,\n",
    "    'lambda': 0.01 ,\n",
    "    'alpha': 0.01 ,\n",
    "    'eval_metric': 'error',\n",
    "}\n",
    "num_round = 1500\n",
    "\n",
    "\n",
    "# prepare values to split up\n",
    "X = train.question_text\n",
    "y = train.target\n",
    "y_preds = np.zeros((X.shape[0], 6))\n",
    "\n",
    "model_components = {}\n",
    "\n",
    "# Loop through K folds and run models\n",
    "for i, (train_index, test_index) in enumerate(kfd.split(X)):\n",
    "    # models\n",
    "    nb = BernoulliNB()\n",
    "    sgd = SGDClassifier(loss='modified_huber', max_iter=1000, tol=1e-2)\n",
    "    svm = SGDClassifier(loss='hinge', max_iter=1000, tol=1e-2)\n",
    "    lr = SGDClassifier(loss='log', max_iter=1000, tol=1e-2)\n",
    "    \n",
    "    print('\\nRunning fold {}\\n'.format(i+1))\n",
    "    \n",
    "    # kfold splits\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # word vectors for NN \n",
    "    print('\\tConvert word vectors to set of arrays')\n",
    "    test_vects = np.array([text_to_array(X_text, embeddings_index) for X_text in X_test])\n",
    "    \n",
    "    # RNN\n",
    "    print('\\tModel {}, RNN'.format(6)) \n",
    "    rnn = NN(layers=32, shape=(30, 900), model_type='GRU')\n",
    "    mg = batch_gen(X_train, y_train)\n",
    "    rnn.model.fit_generator(\n",
    "        mg, \n",
    "        epochs=20,\n",
    "        shuffle=True,\n",
    "        steps_per_epoch=1000,\n",
    "        validation_data=(test_vects, y_test),\n",
    "        validation_steps=100,\n",
    "        verbose=True,\n",
    "#         callbacks=[EarlyStopping()],\n",
    "    )\n",
    "    y_preds[test_index, 5] = rnn.model.predict(test_vects).ravel()\n",
    "#     model_components['rnn'] = rnn\n",
    "    del rnn\n",
    "    del test_vects\n",
    "    del mg\n",
    "    \n",
    "    print('TFIDF transform')\n",
    "    # transform tfidf\n",
    "    tf = TfidfVectorizer(\n",
    "        max_features=5000, \n",
    "        ngram_range=(1,2), \n",
    "        tokenizer=tokenize,\n",
    "    )\n",
    "    X_train = tf.fit_transform(X_train)\n",
    "    X_test = tf.transform(X_test)\n",
    "#     model_components['tf'] = tf\n",
    "    if i < 3:\n",
    "        del tf\n",
    "    \n",
    "    # NB\n",
    "    print('\\tModel {}, Naive Bayes'.format(1)) \n",
    "    nb.fit(X_train, y_train)\n",
    "    y_preds[test_index, 0] = nb.predict_proba(X_test)[:, 1]\n",
    "#     model_components['nb'] = nb\n",
    "    del nb\n",
    "    \n",
    "    # Huber\n",
    "    print('\\tModel {}, Huber'.format(2))\n",
    "    sgd.fit(X_train, y_train)\n",
    "    y_preds[test_index, 1] = sgd.predict(X_test)\n",
    "#     model_components['sgd'] = sgd\n",
    "    del sgd\n",
    "    \n",
    "    # SVM\n",
    "    print('\\tModel {}, SVM'.format(3))\n",
    "    svm.fit(X_train, y_train)\n",
    "    y_preds[test_index, 2] = svm.predict(X_test)\n",
    "#     model_components['svm'] = svm\n",
    "    del svm\n",
    "    \n",
    "    # PCA projection into smaller space\n",
    "#     print('\\tPCA transform')\n",
    "#     pca = TruncatedSVD(n_components=1000)\n",
    "#     X_train = pca.fit_transform(X_train)\n",
    "#     X_test = pca.transform(X_test)\n",
    "#     model_components['pca'] = pca\n",
    "    \n",
    "    # LR\n",
    "    print('\\tModel {}, Logistic Regression'.format(4))\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_preds[test_index, 3] = lr.predict_proba(X_test)[:, 1]\n",
    "#     model_components['lr'] = lr\n",
    "    del lr\n",
    "    \n",
    "    # XGB\n",
    "#     print('\\tModel {}, XGB'.format(5)) \n",
    "#     param['scale_pos_weight'] = np.sum(y_train == 0)*1. / np.sum(y_train == 1)\n",
    "#     dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "#     dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "#     eval_set = [\n",
    "#         (dtrain,'train'), \n",
    "#         (dtest,'test'),\n",
    "#     ]\n",
    "#     bst = xgb.train(\n",
    "#         param, \n",
    "#         dtrain, \n",
    "#         num_round, \n",
    "#         evals=eval_set, \n",
    "#         verbose_eval=200, \n",
    "#         early_stopping_rounds=50\n",
    "#     )\n",
    "#     y_preds[test_index, 4] = bst.predict(dtest)\n",
    "#     model_components['xgb'] = xgb\n",
    "\n",
    "    # LGB\n",
    "    print('\\tModel {}, LGB'.format(5)) \n",
    "    dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "    dtest = lgb.Dataset(X_test, label=y_test)\n",
    "    param = {\n",
    "        'num_leaves':35, \n",
    "        'num_trees':1000, \n",
    "        'objective':'binary',\n",
    "        'min_data_in_leaf': 50 ,\n",
    "        'bagging_fraction': 0.85 ,\n",
    "    }\n",
    "    param['metric'] = 'binary_logloss'\n",
    "    num_round = 100\n",
    "    bst = lgb.train(\n",
    "        param, \n",
    "        dtrain, \n",
    "        num_round, \n",
    "        valid_sets=[dtest], \n",
    "        valid_names=['eval'],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=100,\n",
    "    )\n",
    "    y_preds[test_index, 4] = bst.predict(X_test)\n",
    "#     model_components['lgb'] = lgb\n",
    "    del bst\n",
    "    \n",
    "    # gc\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y, y_preds[:, 0] > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y, y_preds[:, 1] > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y, y_preds[:, 2] > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y, y_preds[:, 3] > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y, y_preds[:, 4] > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y, y_preds[:, 5] > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.653256236576904"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stack models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfd2 = KFold(n_splits=4, random_state=23)\n",
    "y_val = np.zeros(X.shape[0])\n",
    "\n",
    "param = {\n",
    "    'max_depth':4 , \n",
    "    'eta':0.05 , \n",
    "    'silent':1 , \n",
    "    'objective':'binary:logistic' ,\n",
    "    'colsample_bytree': 0.9 ,\n",
    "    'subsample': 0.9 ,\n",
    "    'lambda': 0.01 ,\n",
    "    'alpha': 0.01 ,\n",
    "    'eval_metric': 'error',\n",
    "}\n",
    "num_round = 1500\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kfd2.split(X)):\n",
    "    print('Running fold {}'.format(i+1))\n",
    "\n",
    "    # kfold splits\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    X_train_preds, X_test_preds = y_preds[train_index, :], y_preds[test_index, :]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # transform tfidf\n",
    "    X_train = tf.fit_transform(X_train)\n",
    "    X_test = tf.transform(X_test)\n",
    "    \n",
    "    # append predictions\n",
    "    X_train = sp.sparse.hstack((X_train, sp.sparse.csr_matrix(X_train_preds))).tocsr()\n",
    "    X_test = sp.sparse.hstack((X_test, sp.sparse.csr_matrix(X_test_preds))).tocsr()\n",
    "    \n",
    "    # stacked model\n",
    "    param['scale_pos_weight'] = np.sum(y_train == 0)*1. / np.sum(y_train == 1)\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "    eval_set = [\n",
    "        (dtrain,'train'), \n",
    "        (dtest,'test'),\n",
    "    ]\n",
    "    bst = xgb.train(\n",
    "        param, \n",
    "        dtrain, \n",
    "        num_round, \n",
    "        evals=eval_set, \n",
    "        verbose_eval=200,\n",
    "        early_stopping_rounds=100,\n",
    "        \n",
    "    )\n",
    "    y_val[test_index] = bst.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y, y_val > 0.50) # top leaderboard has f1 score ~0.7+"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Things left to try:\n",
    "- Concatenate other embeddings onto glove (word2vec, etc)\n",
    "- Bayesian parameter optimization for LGB, test different PCA logic\n",
    "- More words in each window\n",
    "- Stack first & last words \n",
    "- punctuation\n",
    "- pseudo labeling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
