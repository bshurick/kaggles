{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RedHat Kaggle\n",
    "\n",
    "https://www.kaggle.com/c/predicting-red-hat-business-value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import multiprocessing\n",
    "from collections import Counter, defaultdict\n",
    "import hashlib\n",
    "import re\n",
    "from sklearn.learning_curve import validation_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "CPUS = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CPUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_col(x, p):\n",
    "    if type(x)==str:\n",
    "        return p+'_'+x.replace(' ','_')\n",
    "    else:\n",
    "        return p+'_'+str(x)\n",
    "\n",
    "def update_cols(df,c=1):\n",
    "    # fix date columns\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # split date column into several \n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['qtr'] = df['date'].dt.month // 3\n",
    "    \n",
    "    # remove date \n",
    "    del df['date']\n",
    "    \n",
    "    # get list of columns\n",
    "    cols = list(df.columns)\n",
    "    \n",
    "    # include column name with value\n",
    "    for p in cols[c:]:\n",
    "        df.loc[:,p] = df.loc[:,p].apply(lambda x: format_col(x, p) )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Brandon C Shurickdef test_scores(y_test, predictions, pr=True):\n",
    "    ll = log_loss(y_test, predictions)\n",
    "    a = accuracy_score(y_test, np.argmax(predictions,axis=1))\n",
    "    auc = roc_auc_score(np.c_[y_test==0, y_test==1], predictions)\n",
    "    r = {'c':c,'logloss':ll,'accuracy':a,'AUC':auc}\n",
    "    pstr = '''C:{c} Log-Loss:{logloss:.7f} Accuracy:{accuracy:.7f} AUC:{AUC:.7f}'''\n",
    "    if pr: print pstr.format(**r)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_sparse_ohe_data(df, ohe_dict):\n",
    "    rows = []\n",
    "    cols = []\n",
    "    data = []\n",
    "    for i in xrange(df.shape[0]):\n",
    "        s = map(lambda x: ohe_dict.get(x,-1), df.iloc[i])\n",
    "        while True:\n",
    "            if -1 in s:\n",
    "                indval = s.index(-1)\n",
    "                del s[indval]\n",
    "            else:\n",
    "                break\n",
    "        rows += [i]*len(s)\n",
    "        cols += s\n",
    "        data += [1]*len(s)\n",
    "    return csr_matrix((np.array(data),\n",
    "                (np.array(rows),\n",
    "                 np.array(cols))),\n",
    "               shape=(df.shape[0],len(ohe_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_sparse_hash_data(df, num_buckets):\n",
    "    rows = []\n",
    "    cols = []\n",
    "    data = []\n",
    "    for i in xrange(df.shape[0]):\n",
    "        mapping = {}\n",
    "        for f in df.iloc[i]:\n",
    "            mapping[f] = int(int(hashlib.md5(f).hexdigest(), 16) % num_buckets)\n",
    "        s = defaultdict(float)\n",
    "        for bucket in mapping.values():\n",
    "            s[bucket] += 1.0\n",
    "        rows += [i]*len(s)\n",
    "        cols += s.keys()\n",
    "        data += s.values()\n",
    "    return csr_matrix((np.array(data),\n",
    "                (np.array(rows),\n",
    "                 np.array(cols))),\n",
    "               shape=(df.shape[0],num_buckets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ClusterLogisticRegression(LogisticRegression):\n",
    "    def __init__(self, n_clusters=10, n_jobs=1, C=1.0, models=[], **params):\n",
    "        self.models = [ LogisticRegression(n_jobs=n_jobs, C=C, **params) \n",
    "                           for i in range(n_clusters) ] if not models else models\n",
    "        self.cluster_model = KMeans(n_clusters=n_clusters, n_jobs=n_jobs)\n",
    "        self.classes = 2\n",
    "        self.C = C\n",
    "        self.n_clusters = n_clusters\n",
    "        self.n_jobs = n_jobs\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        for i in range(self.n_clusters):\n",
    "            self.models[i].set_params(**params)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.cluster_model.fit(X)\n",
    "        clusters = self.cluster_model.labels_\n",
    "        for i in xrange(self.n_clusters):\n",
    "            self.models[i] = self.models[i].fit(X[clusters==i], \n",
    "                                                y[clusters==i])\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        clusters = self.cluster_model.predict(X)\n",
    "        predictions = np.zeros((X.shape[0], self.classes))\n",
    "        for i in xrange(self.n_clusters):\n",
    "            if np.sum(clusters==i)>0:\n",
    "                predictions[clusters==i,:] = self.models[i].predict_proba(X[clusters==i])[:,:]\n",
    "        return predictions\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lr_validation_curve(X, y, \n",
    "                        param_range= np.logspace(-3, 3, 3), \n",
    "                        max_iter=100, \n",
    "                        tol=1e-4, \n",
    "                        cv=5, \n",
    "                        n_clusters=4,\n",
    "                        n_jobs=CPUS):\n",
    "    lr_model = ClusterLogisticRegression(max_iter=max_iter, tol=tol, n_clusters=n_clusters)\n",
    "    train_scores, test_scores = validation_curve(lr_model, X, y, \n",
    "                                param_name=\"C\", param_range=param_range,\n",
    "                                cv=cv, scoring=\"roc_auc\", n_jobs=n_jobs)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.title(\"Validation Curve with Logistic Regression\")\n",
    "    plt.xlabel(\"C\")\n",
    "    plt.ylabel(\"AUC\")\n",
    "    plt.ylim(0.5, 1.1)\n",
    "    plt.semilogx(param_range, train_scores_mean, label=\"Training score\", color=\"r\")\n",
    "    plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.2, color=\"r\")\n",
    "    plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "                 color=\"g\")\n",
    "    plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.2, color=\"g\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DIR = './'\n",
    "FTRAIN = 'Data/act_train.csv.gz'\n",
    "FTEST = 'Data/act_test.csv.gz'\n",
    "FPEOPLE = 'Data/people.csv.gz'\n",
    "FSAMPLE = 'Data/sample_submission.csv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv(DIR+FTRAIN)\n",
    "test_raw = pd.read_csv(DIR+FTEST)\n",
    "people = pd.read_csv(DIR+FPEOPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2197291, 15)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(498687, 14)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(189118, 41)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set date types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_raw['date'] = pd.to_datetime(train_raw['date'])\n",
    "test_raw['date'] = pd.to_datetime(test_raw['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Activity history reference file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type 2    904683\n",
       "type 5    490710\n",
       "type 3    429408\n",
       "type 4    207465\n",
       "type 1    157615\n",
       "type 6      4253\n",
       "type 7      3157\n",
       "Name: activity_category, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw['activity_category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def gather_activity_history(values):\n",
    "    for v in values:\n",
    "        yield train_raw.groupby(level=0)['activity_category'].apply(lambda x: np.max(x==v))\n",
    "\n",
    "values = set( train_raw['activity_category'].values )\n",
    "histories = gather_activity_history( values )\n",
    "activity_history = pd.concat( histories, axis=1 )\n",
    "activity_history.columns = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gather minimum dates for each person-category\n",
    "r = train_raw[['people_id',\n",
    "               'date',\n",
    "               'activity_category']].groupby(['people_id',\n",
    "                                              'activity_category']\n",
    "                                            ).apply(lambda x: np.min(x.date))\n",
    "r = r.reset_index()\n",
    "r.columns = ['people_id','activity_category','min_date']\n",
    "\n",
    "# merge with df data\n",
    "t = train_raw.loc[:,['people_id','activity_category','date']]\n",
    "tm = pd.merge(t, r, on=['people_id','activity_category'], how='outer')\n",
    "\n",
    "# create column for each category type \n",
    "def gather_history_cols(values):\n",
    "    for v in sorted(values):\n",
    "        check_func = lambda x: np.max((x.activity_category==v)&(x.date>=x.min_date))\n",
    "        yield tm.groupby(['people_id','date']).apply(check_func)\n",
    "values = set( train_raw.activity_category.values )\n",
    "activity_history = pd.concat(gather_history_cols(values), axis=1)*1\n",
    "activity_history.columns = map(lambda x: 'h'+str(x),range(len(list(values))))\n",
    "activity_history = activity_history.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Date probability reference file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date_probs = train_raw.groupby('date')['outcome'].apply(np.mean)\n",
    "date_probs.columns = ['date_prob']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Outcome variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = train_raw['outcome']\n",
    "del train_raw['outcome']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = Y.value_counts()*1.0 / train_raw.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = train_raw.shape[0]\n",
    "predictions = np.zeros((N,2))\n",
    "for i in xrange(N):\n",
    "    predictions[i,:] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55604560342712916"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y, np.argmax(predictions,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68685173924161147"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(Y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(np.c_[Y==0,Y==1], predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make numeric features categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "percentile_ref = people['char_38'].ravel()\n",
    "compare_percentile = lambda x: round(sp.stats.percentileofscore(percentile_ref, x, kind='weak'),-1)\n",
    "people['char_38'] = people['char_38'].apply(compare_percentile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add date probability columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "percentile_ref = date_probs.ravel()\n",
    "compare_percentile = lambda x: round(sp.stats.percentileofscore(percentile_ref, x, kind='weak'),-1)\n",
    "date_probs_df = pd.DataFrame()\n",
    "date_probs_df['date_prob_pctl'] = date_probs.apply(compare_percentile)\n",
    "date_probs_df = date_probs_df.reset_index()\n",
    "date_probs_df['date'] = pd.to_datetime(date_probs_df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_date_probs(df, date_probs):\n",
    "    df = pd.merge(df, date_probs, \n",
    "                  how='left', on='date')\n",
    "    return df\n",
    "\n",
    "train_raw = add_date_probs(train_raw, date_probs_df)\n",
    "test_raw = add_date_probs(test_raw, date_probs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add activity history columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_history_cols(df, activity_history):\n",
    "    df = pd.merge(df, activity_history,\n",
    "                 how='left', on=['people_id','date'])\n",
    "    return df\n",
    "\n",
    "train_raw = add_history_cols(train_raw, activity_history)\n",
    "test_raw = add_history_cols(test_raw, activity_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reindex and clear memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_raw.index = train_raw['people_id']\n",
    "test_raw.index = test_raw['people_id']\n",
    "people.index = people['people_id']\n",
    "del train_raw['people_id']\n",
    "del test_raw['people_id']\n",
    "del people['people_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# updated people cols\n",
    "people = update_cols(people, 0)\n",
    "\n",
    "# train features\n",
    "train_raw = update_cols(train_raw, 1)\n",
    "\n",
    "# test features\n",
    "test_raw = update_cols(test_raw, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transform data to sparse OHE matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_full = pd.merge(train_raw, people, left_index=True, right_index=True)\n",
    "test_full = pd.merge(test_raw, people, left_index=True, right_index=True)\n",
    "train_full.index = train_full['activity_id']\n",
    "test_full.index = test_full['activity_id']\n",
    "del train_full['activity_id']\n",
    "del test_full['activity_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1296054646372795"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train_full.memory_usage())*1.0/1024**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25637003034353256"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(test_full.memory_usage())*1.0/1024**3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create OHE dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_values = np.unique(train_full.as_matrix().reshape(1,-1).ravel())\n",
    "ohe_dict= { k: v for k,v in zip(all_values, xrange(all_values.shape[0])) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36809"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ohe_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create sparse matrices for test and training datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = create_sparse_ohe_data(train_full, ohe_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test = create_sparse_ohe_data(test_full, ohe_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IDs = np.array(test_full.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2197291, 36809)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clear unused memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del train_full \n",
    "del train_raw\n",
    "del test_full\n",
    "del test_raw\n",
    "del people \n",
    "del ohe_dict\n",
    "del all_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split train and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, \n",
    "                                                  Y, \n",
    "                                                  test_size=0.20, \n",
    "                                                  random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check model parameter tuning\n",
    "Using a sample of data to limit runtime.  \n",
    "Assumes that sampled validation curve will match full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "sample = np.random.choice(xrange(X.shape[0]),size=100000)\n",
    "lr_validation_curve(X[sample,:], \n",
    "                    Y.iloc[sample], \n",
    "                    param_range= np.logspace(-3, 5, 6), \n",
    "                    n_clusters=6,\n",
    "                    n_jobs=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = X.shape[0]\n",
    "e = N/10*8\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.2)\n",
    "X_train, X_val, y_train, y_val = X[:e,:], X[e:,:], Y[:e], Y[e:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = 1e2\n",
    "lr = LogisticRegression(C=c, max_iter=200, tol=1e-4, n_jobs=CPUS)\n",
    "lr.fit(X_train, y_train)\n",
    "predictions = lr.predict_proba(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:100.0 Log-Loss:0.2023555 Accuracy:0.9089653 AUC:0.9770715\n"
     ]
    }
   ],
   "source": [
    "_ = test_scores(y_val, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "<tr>\n",
    "    <th>Iter</th>\n",
    "    <th>C</th>\n",
    "    <th>Clusters</th>\n",
    "    <th>Tol</th>\n",
    "    <th>+Features</th>\n",
    "    <th>AUC</th>\n",
    "    <th>Log Loss</th>\n",
    "    <th>Accuracy</th>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>200</td>\n",
    "    <td>100</td>\n",
    "    <td>1</td>\n",
    "    <td>1e-5</td>\n",
    "    <td></td>\n",
    "    <td>0.9767696</td>\n",
    "    <td>0.2064201</td>\n",
    "    <td>0.9075454</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>200</td>\n",
    "    <td>100</td>\n",
    "    <td>1</td>\n",
    "    <td>1e-5</td>\n",
    "    <td>+Activity history</td>\n",
    "    <td>0.9767806</td>\n",
    "    <td>0.2046148</td>\n",
    "    <td>0.9075249</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>200</td>\n",
    "    <td>100</td>\n",
    "    <td>1</td>\n",
    "    <td>1e-5</td>\n",
    "    <td>+Date Prob</td>\n",
    "    <td>0.9768128</td>\n",
    "    <td>0.2063661</td>\n",
    "    <td>0.9080028</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>300</td>\n",
    "    <td>100</td>\n",
    "    <td>1</td>\n",
    "    <td>1e-5</td>\n",
    "    <td>+More accurate history</td>\n",
    "    <td>0.9768163</td>\n",
    "    <td>0.2062309</td>\n",
    "    <td>0.9082030</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>200</td>\n",
    "    <td>100</td>\n",
    "    <td>1</td>\n",
    "    <td>1e-4</td>\n",
    "    <td>-Iterations and tol</td>\n",
    "    <td>0.9770576</td>\n",
    "    <td>0.2029024</td>\n",
    "    <td>0.9088720</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>200</td>\n",
    "    <td>100</td>\n",
    "    <td>1</td>\n",
    "    <td>1e-3</td>\n",
    "    <td>-more tol</td>\n",
    "    <td>0.9766761</td>\n",
    "    <td>0.1920801</td>\n",
    "    <td>0.9106515</td>\n",
    "</tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "F = (0.15*lr.coef_.shape[1])\n",
    "A = np.argsort(np.abs(lr.coef_))[:,::-1].ravel()[:-F]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = X.shape[0]\n",
    "e = N/10*8\n",
    "X_train, X_val, y_train, y_val = X[:e,A], X[e:,A], Y[:e], Y[e:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = 1e2\n",
    "lr = LogisticRegression(C=c, max_iter=200, tol=1e-4, n_jobs=CPUS)\n",
    "lr.fit(X_train, y_train)\n",
    "predictions = lr.predict_proba(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = test_scores(y_val, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c     = 1e2\n",
    "tol   = 1e-4\n",
    "iters = 200\n",
    "lr = LogisticRegression(C=c, max_iter=iters, tol=tol, n_jobs=CPUS)\n",
    "lr.fit(X, Y)\n",
    "predictions = lr.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(np.vstack((IDs, \n",
    "                                     predictions[:,1])).T,\n",
    "                          columns=['activity_id','outcome'])\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use already-known predictions via forums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_leak_data(submission, loc='./Data/leak.csv.gz'):\n",
    "    leak = pd.read_csv(loc)\n",
    "    leak.columns = ['activity_id','leak_outcome']\n",
    "    leak = leak.loc[(leak['leak_outcome']==1.0) | (leak['leak_outcome']==0.0),:]\n",
    "    leak_submission = pd.merge(submission, leak, how='left', on='activity_id')\n",
    "    nonleak = np.isnan(leak_submission['leak_outcome'])\n",
    "    leak_submission.loc[nonleak,'leak_outcome'] = leak_submission.loc[nonleak, 'outcome']\n",
    "    del leak_submission['outcome']\n",
    "    leak_submission.columns = ['activity_id','outcome']\n",
    "    return leak_submission\n",
    "\n",
    "# leak_submission = add_leak_data(submission)\n",
    "# leak_submission.to_csv('leak_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!gzip submission.csv\n",
    "!s3put -bbrandonshurick -p/home/ubuntu/ -gpublic-read submission.csv.gz "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_scores_xgb(y_val, predictions):\n",
    "    pfull = np.c_[1-predictions,predictions]\n",
    "    ll = log_loss(y_val, pfull)\n",
    "    a = accuracy_score(y_val, np.argmax(pfull ,axis=1))\n",
    "    auc = roc_auc_score(np.c_[y_val==0, y_val==1], pfull)\n",
    "    r = {'logloss':ll,'accuracy':a,'AUC':auc}\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix( X_train, label=y_train.as_matrix() )\n",
    "dval = xgb.DMatrix( X_val, label=y_val.as_matrix() )\n",
    "dtest = xgb.DMatrix( X_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-logloss:0.283932\ttrain-logloss:0.18327\n",
      "[50]\teval-logloss:0.195622\ttrain-logloss:0.080249\n",
      "[100]\teval-logloss:0.195373\ttrain-logloss:0.080235\n"
     ]
    }
   ],
   "source": [
    "d = 5\n",
    "e = 0.01\n",
    "param = {'bst:max_depth':d, \n",
    "         'bst:eta':e, \n",
    "         'subsample':0.5, \n",
    "         'colsample_bytree':0.7,\n",
    "         'silent':1, \n",
    "         'lambda':1.0,\n",
    "         'objective':'binary:logistic',\n",
    "         'min_child_weight':0,\n",
    "         'booster':'gblinear'}\n",
    "param['eval_metric'] = 'logloss'\n",
    "param['nthread'] = CPUS\n",
    "evallist  = [(dval,'eval'), (dtrain,'train')]\n",
    "xgb_model = xgb.train(param.items(), dtrain, 101, evallist, verbose_eval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AUC': 0.97440198519505272,\n",
       " 'accuracy': 0.90791404886462679,\n",
       " 'logloss': 0.19537269320285294}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = xgb_model.predict( dval )\n",
    "test_scores_xgb(y_val, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = xgb_model.predict( dtest )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(np.vstack((IDs, \n",
    "                                     predictions)).T,\n",
    "                          columns=['activity_id','outcome'])\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "leak_submission = add_leak_data(submission)\n",
    "leak_submission.to_csv('leak_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leak_submission.csv.gz already exists -- do you wish to overwrite (y or n)? ^C\n"
     ]
    }
   ],
   "source": [
    "!gzip leak_submission.csv "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
