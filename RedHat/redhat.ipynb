{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RedHat Kaggle\n",
    "\n",
    "https://www.kaggle.com/c/predicting-red-hat-business-value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pandas\n",
    "import pandas as pd\n",
    "\n",
    "# scipy \n",
    "import scipy as sp\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "# sklearn models \n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import tree\n",
    "\n",
    "# scoring and cross validation \n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, r2_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.learning_curve import validation_curve\n",
    "\n",
    "# multiprocessing\n",
    "import multiprocessing\n",
    "CPUS = multiprocessing.cpu_count()\n",
    "\n",
    "# collections \n",
    "from collections import Counter, defaultdict\n",
    "from copy import copy\n",
    "\n",
    "# hashing\n",
    "import hashlib\n",
    "\n",
    "# regular expressions \n",
    "import re\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# pickle\n",
    "import pickle\n",
    "\n",
    "# logging \n",
    "import logging\n",
    "logging.basicConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Num CPUs: 8\n"
     ]
    }
   ],
   "source": [
    "logging.warn('Num CPUs: {}'.format(CPUS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DIR = './'\n",
    "FTRAIN = 'Data/act_train.csv.gz'\n",
    "FTEST = 'Data/act_test.csv.gz'\n",
    "FPEOPLE = 'Data/people.csv.gz'\n",
    "FSAMPLE = 'Data/sample_submission.csv.gz'\n",
    "SPARSE_TRAIN_FILE = DIR+'Data/sparse_train.dat'\n",
    "SPARSE_TEST_FILE = DIR+'Data/sparse_test.dat'\n",
    "ID_FILE = DIR+'Data/ids.dat'\n",
    "Y_FILE = DIR+'Data/y.dat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ClusterLogisticRegression(LogisticRegression):\n",
    "    ''' Create separate LR models for each KMeans cluster '''\n",
    "    def __init__(self, n_clusters=10, n_jobs=1, C=1.0, models=[], **params):\n",
    "        self.models = [ LogisticRegression(n_jobs=n_jobs, C=C, **params) \n",
    "                           for i in range(n_clusters) ] if not models else models\n",
    "        self.cluster_model = KMeans(n_clusters=n_clusters, n_jobs=n_jobs)\n",
    "        self.classes = 2\n",
    "        self.C = C\n",
    "        self.n_clusters = n_clusters\n",
    "        self.n_jobs = n_jobs\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        for i in range(self.n_clusters):\n",
    "            self.models[i].set_params(**params)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.cluster_model.fit(X)\n",
    "        clusters = self.cluster_model.labels_\n",
    "        for i in xrange(self.n_clusters):\n",
    "            self.models[i] = self.models[i].fit(X[clusters==i], \n",
    "                                                y[clusters==i])\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        clusters = self.cluster_model.predict(X)\n",
    "        predictions = np.zeros((X.shape[0], self.classes))\n",
    "        for i in xrange(self.n_clusters):\n",
    "            if np.sum(clusters==i)>0:\n",
    "                predictions[clusters==i,:] = self.models[i].predict_proba(X[clusters==i])[:,:]\n",
    "        return predictions\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_col(x, p):\n",
    "    ''' Add column header to each row value '''\n",
    "    if type(x)==str:\n",
    "        return p+'_'+x.replace(' ','_')\n",
    "    else:\n",
    "        return p+'_'+str(x)\n",
    "\n",
    "def update_cols(df,c=1):\n",
    "    ''' Transform date columns \n",
    "        and run format_col on each col\n",
    "    '''\n",
    "    # fix date columns\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # split date column into several \n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['qtr'] = df['date'].dt.month // 3\n",
    "    \n",
    "    # remove date \n",
    "    del df['date']\n",
    "    \n",
    "    # get list of columns\n",
    "    cols = list(df.columns)\n",
    "    \n",
    "    # include column name with value\n",
    "    for p in cols[c:]:\n",
    "        df.loc[:,p] = df.loc[:,p].apply(lambda x: format_col(x, p) )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_scores(y_test, predictions, pr=True):\n",
    "    ''' Output test scores for predictions '''\n",
    "    ll = log_loss(y_test, predictions)\n",
    "    a = accuracy_score(y_test, np.argmax(predictions,axis=1))\n",
    "    auc = roc_auc_score(np.c_[y_test==0, y_test==1], predictions)\n",
    "    r = {'logloss':ll,'accuracy':a,'AUC':auc}\n",
    "    pstr = '''Log-Loss:{logloss:.7f} Accuracy:{accuracy:.7f} AUC:{AUC:.7f}'''\n",
    "    if pr: print pstr.format(**r)\n",
    "    return r\n",
    "\n",
    "# quick RMSE for regression results\n",
    "RMSE = lambda x,y: np.sqrt(np.mean(x-y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_sparse_ohe_data(df, ohe_dict):\n",
    "    ''' Create a sparsified OHE matrix '''\n",
    "    rows = []\n",
    "    cols = []\n",
    "    data = []\n",
    "    for i in xrange(df.shape[0]):\n",
    "        s = map(lambda x: ohe_dict.get(x,-1), df.iloc[i])\n",
    "        while True:\n",
    "            if -1 in s:\n",
    "                indval = s.index(-1)\n",
    "                del s[indval]\n",
    "            else:\n",
    "                break\n",
    "        rows += [i]*len(s)\n",
    "        cols += s\n",
    "        data += [1]*len(s)\n",
    "    return csr_matrix((np.array(data),\n",
    "                (np.array(rows),\n",
    "                 np.array(cols))),\n",
    "               shape=(df.shape[0],len(ohe_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_sparse_hash_data(df, num_buckets):\n",
    "    ''' Group raw features together into \n",
    "        hash bucketed features and represent\n",
    "        as a sparse matrix\n",
    "    '''\n",
    "    rows = []\n",
    "    cols = []\n",
    "    data = []\n",
    "    for i in xrange(df.shape[0]):\n",
    "        mapping = {}\n",
    "        for f in df.iloc[i]:\n",
    "            mapping[f] = int(int(hashlib.md5(f).hexdigest(), 16) % num_buckets)\n",
    "        s = defaultdict(float)\n",
    "        for bucket in mapping.values():\n",
    "            s[bucket] += 1.0\n",
    "        rows += [i]*len(s)\n",
    "        cols += s.keys()\n",
    "        data += s.values()\n",
    "    return csr_matrix((np.array(data),\n",
    "                (np.array(rows),\n",
    "                 np.array(cols))),\n",
    "               shape=(df.shape[0],num_buckets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lr_validation_curve(X, y, \n",
    "                        param_range= np.logspace(-3, 3, 3), \n",
    "                        max_iter=100, \n",
    "                        tol=1e-4, \n",
    "                        cv=5,\n",
    "                        n_jobs=CPUS):\n",
    "    ''' Output the validation curve for LR model '''\n",
    "    lr_model = LogisticRegression(max_iter=max_iter, tol=tol)\n",
    "    train_scores, test_scores = validation_curve(lr_model, X, y, \n",
    "                                param_name=\"C\", param_range=param_range,\n",
    "                                cv=cv, scoring=\"roc_auc\", n_jobs=n_jobs)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.title(\"Validation Curve with Logistic Regression\")\n",
    "    plt.xlabel(\"C\")\n",
    "    plt.ylabel(\"AUC\")\n",
    "    plt.ylim(0.5, 1.1)\n",
    "    plt.semilogx(param_range, train_scores_mean, label=\"Training score\", color=\"r\")\n",
    "    plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.2, color=\"r\")\n",
    "    plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "                 color=\"g\")\n",
    "    plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.2, color=\"g\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dump_files(X, X_test, Y, IDS):\n",
    "    ''' Output data to pickle file '''\n",
    "    with open(SPARSE_TRAIN_FILE, 'wb') as w:\n",
    "        pickle.dump(X, w)\n",
    "\n",
    "    with open(SPARSE_TEST_FILE, 'wb') as w:\n",
    "        pickle.dump(X_test, w)\n",
    "\n",
    "    with open(ID_FILE, 'wb') as w:\n",
    "        pickle.dump(IDS, w)\n",
    "    \n",
    "    with open(Y_FILE, 'wb') as w:\n",
    "        pickle.dump(Y, w)\n",
    "\n",
    "def read_files():\n",
    "    ''' Read from pickle dat files '''\n",
    "    global X\n",
    "    global X_test\n",
    "    global IDs\n",
    "    global Y\n",
    "    \n",
    "    with open(SPARSE_TRAIN_FILE, 'rb') as w:\n",
    "        X = pickle.load(w)\n",
    "\n",
    "    with open(SPARSE_TEST_FILE, 'rb') as w:\n",
    "        X_test = pickle.load(w)\n",
    "\n",
    "    with open(ID_FILE, 'rb') as w:\n",
    "        IDs = pickle.load(w) \n",
    "    \n",
    "    with open(Y_FILE, 'rb') as w:\n",
    "        Y = pickle.load(w)\n",
    "\n",
    "def check_exists(x_str):\n",
    "    ''' Check if an object exists '''\n",
    "    x_exists = x_str in locals() or x_str in globals()\n",
    "    return x_exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv(DIR+FTRAIN)\n",
    "test_raw = pd.read_csv(DIR+FTEST)\n",
    "people = pd.read_csv(DIR+FPEOPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2197291, 15)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(498687, 14)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(189118, 41)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set date types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_raw['date'] = pd.to_datetime(train_raw['date'])\n",
    "test_raw['date'] = pd.to_datetime(test_raw['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x10bd5b310>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmQAAAEwCAYAAAAZ5m3AAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ab88b10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 3))\n",
    "f = train_raw[['activity_category','outcome']].groupby('activity_category').mean()\n",
    "f = f.reset_index(level=0)\n",
    "sns.barplot(x='activity_category',y='outcome',data=f,color='steelblue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = train_raw[['people_id','outcome']].groupby('people_id').sum()\n",
    "f['total_activity_cnt'] = train_raw[['people_id','outcome']].groupby('people_id').count()\n",
    "f['pct'] = f['outcome'] / f['total_activity_cnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median outcome: 0.0\n",
      "Median activity count: 5.0\n",
      "% with any positive outcomes: 45.45%\n"
     ]
    }
   ],
   "source": [
    "print 'Median outcome: {}'.format(np.median(f['outcome']))\n",
    "print 'Median activity count: {}'.format(np.median(f['total_activity_cnt']))\n",
    "print '% with any positive outcomes: {}%'.format(round(100*np.mean(f['outcome']>0),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>outcome</th>\n",
       "      <th>total_activity_cnt</th>\n",
       "      <th>pct</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>people_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ppl_10006</th>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ppl_100075</th>\n",
       "      <td>2</td>\n",
       "      <td>59</td>\n",
       "      <td>0.033898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ppl_100145</th>\n",
       "      <td>36</td>\n",
       "      <td>38</td>\n",
       "      <td>0.947368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ppl_100297</th>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>0.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ppl_100324</th>\n",
       "      <td>67</td>\n",
       "      <td>81</td>\n",
       "      <td>0.827160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ppl_100382</th>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ppl_100387</th>\n",
       "      <td>399</td>\n",
       "      <td>450</td>\n",
       "      <td>0.886667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ppl_10041</th>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ppl_100451</th>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ppl_100510</th>\n",
       "      <td>21</td>\n",
       "      <td>47</td>\n",
       "      <td>0.446809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            outcome  total_activity_cnt       pct\n",
       "people_id                                        \n",
       "ppl_10006        10                  11  0.909091\n",
       "ppl_100075        2                  59  0.033898\n",
       "ppl_100145       36                  38  0.947368\n",
       "ppl_100297       13                  16  0.812500\n",
       "ppl_100324       67                  81  0.827160\n",
       "ppl_100382       12                  15  0.800000\n",
       "ppl_100387      399                 450  0.886667\n",
       "ppl_10041         4                  24  0.166667\n",
       "ppl_100451        5                  13  0.384615\n",
       "ppl_100510       21                  47  0.446809"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[(f['pct']!=0) & (f['pct']!=1)].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>people_id</th>\n",
       "      <th>date</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2861</th>\n",
       "      <td>ppl_100451</td>\n",
       "      <td>2022-08-18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2862</th>\n",
       "      <td>ppl_100451</td>\n",
       "      <td>2022-08-27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2866</th>\n",
       "      <td>ppl_100451</td>\n",
       "      <td>2022-09-14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2872</th>\n",
       "      <td>ppl_100451</td>\n",
       "      <td>2022-09-14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2873</th>\n",
       "      <td>ppl_100451</td>\n",
       "      <td>2022-09-15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2865</th>\n",
       "      <td>ppl_100451</td>\n",
       "      <td>2022-11-16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2864</th>\n",
       "      <td>ppl_100451</td>\n",
       "      <td>2023-01-26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2868</th>\n",
       "      <td>ppl_100451</td>\n",
       "      <td>2023-01-27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2863</th>\n",
       "      <td>ppl_100451</td>\n",
       "      <td>2023-03-08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2869</th>\n",
       "      <td>ppl_100451</td>\n",
       "      <td>2023-03-08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2870</th>\n",
       "      <td>ppl_100451</td>\n",
       "      <td>2023-05-25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2871</th>\n",
       "      <td>ppl_100451</td>\n",
       "      <td>2023-05-25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2867</th>\n",
       "      <td>ppl_100451</td>\n",
       "      <td>2023-05-26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       people_id       date  outcome\n",
       "2861  ppl_100451 2022-08-18        1\n",
       "2862  ppl_100451 2022-08-27        1\n",
       "2866  ppl_100451 2022-09-14        1\n",
       "2872  ppl_100451 2022-09-14        1\n",
       "2873  ppl_100451 2022-09-15        1\n",
       "2865  ppl_100451 2022-11-16        0\n",
       "2864  ppl_100451 2023-01-26        0\n",
       "2868  ppl_100451 2023-01-27        0\n",
       "2863  ppl_100451 2023-03-08        0\n",
       "2869  ppl_100451 2023-03-08        0\n",
       "2870  ppl_100451 2023-05-25        0\n",
       "2871  ppl_100451 2023-05-25        0\n",
       "2867  ppl_100451 2023-05-26        0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = (train_raw['people_id']=='ppl_100451')\n",
    "train_raw[p][['people_id','date','outcome']].sort_values(by='date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create predictive model for people positive outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34470\n",
      "SVM accuracy: 0.846887337845\n"
     ]
    }
   ],
   "source": [
    "# create outcome variable to predict \n",
    "pY = (f['outcome']>0)*1\n",
    "\n",
    "# merge copy of people data with new outcome variable\n",
    "p = pd.merge(people.copy(), pd.DataFrame(pY.ravel(),\n",
    "                              columns=['has_outcome'],\n",
    "                              index=pY.index),\n",
    "                              how='left',\n",
    "                              left_on='people_id',\n",
    "                              right_index=True).fillna(0)\n",
    "_Y = p['has_outcome'].ravel()\n",
    "\n",
    "# save ids \n",
    "_IDs = p['people_id']\n",
    "\n",
    "# clear unused features\n",
    "del p['people_id']\n",
    "del p['has_outcome'] \n",
    "del p['char_38'] # excluding only integer value\n",
    "_X = p\n",
    "del p\n",
    "\n",
    "# create ohe dict for sparse matrix transform\n",
    "_X = update_cols(_X)\n",
    "all_values = set(_X.as_matrix().reshape(1,-1).ravel())\n",
    "ohe_dict= { k: v for k,v in zip(all_values, xrange(len(all_values))) }\n",
    "print len(all_values)\n",
    "\n",
    "# create sparse OHE matrix \n",
    "_X = create_sparse_ohe_data(_X, ohe_dict)\n",
    "\n",
    "# split test and train\n",
    "_X_train, _X_val, _y_train, _y_val = train_test_split(_X, \n",
    "                                                      _Y.T, \n",
    "                                                      test_size=0.3)\n",
    "\n",
    "# fit svm model\n",
    "svm = LinearSVC(C=1.0)\n",
    "svm.fit(_X_train, _y_train)\n",
    "\n",
    "# measure accuracy\n",
    "print 'SVM accuracy: {}'.format(np.mean(svm.predict(_X_val) == _y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outcome_feature = pd.DataFrame(svm.predict(_X), index=_IDs, columns=['outcome_feature'])\n",
    "people = pd.merge(people, outcome_feature,\n",
    "                  how='left', left_on='people_id', right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Activity history reference file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type 2    904683\n",
       "type 5    490710\n",
       "type 3    429408\n",
       "type 4    207465\n",
       "type 1    157615\n",
       "type 6      4253\n",
       "type 7      3157\n",
       "Name: activity_category, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw['activity_category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def gather_activity_history(values):\n",
    "    for v in values:\n",
    "        yield train_raw.groupby(level=0)['activity_category'].apply(lambda x: np.max(x==v))\n",
    "\n",
    "values = set( train_raw['activity_category'].values )\n",
    "histories = gather_activity_history( values )\n",
    "activity_history = pd.concat( histories, axis=1 )\n",
    "activity_history.columns = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gather minimum dates for each person-category\n",
    "r = train_raw[['people_id',\n",
    "               'date',\n",
    "               'activity_category']].groupby(['people_id',\n",
    "                                              'activity_category']\n",
    "                                            ).apply(lambda x: np.min(x.date))\n",
    "r = r.reset_index()\n",
    "r.columns = ['people_id','activity_category','min_date']\n",
    "\n",
    "# merge with df data\n",
    "t = train_raw.loc[:,['people_id','activity_category','date']]\n",
    "tm = pd.merge(t, r, on=['people_id','activity_category'], how='outer')\n",
    "\n",
    "# create column for each category type \n",
    "def gather_history_cols(values):\n",
    "    for v in sorted(values):\n",
    "        check_func = lambda x: np.max((x.activity_category==v)&(x.date>=x.min_date))\n",
    "        yield tm.groupby(['people_id','date']).apply(check_func)\n",
    "values = set( train_raw.activity_category.values )\n",
    "activity_history = pd.concat(gather_history_cols(values), axis=1)*1\n",
    "activity_history.columns = map(lambda x: 'h'+str(x),range(len(list(values))))\n",
    "activity_history = activity_history.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Date probability reference file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date_probs = train_raw.groupby('date')['outcome'].apply(np.mean)\n",
    "date_probs.columns = ['date_prob']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Outcome variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = train_raw['outcome']\n",
    "del train_raw['outcome']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make numeric features categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "percentile_ref = people['char_38'].ravel()\n",
    "compare_percentile = lambda x: round(sp.stats.percentileofscore(percentile_ref, x, kind='weak'),-1)\n",
    "people['char_38'] = people['char_38'].apply(compare_percentile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add date probability columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "percentile_ref = date_probs.ravel()\n",
    "compare_percentile = lambda x: round(sp.stats.percentileofscore(percentile_ref, x, kind='weak'),-1)\n",
    "date_probs_df = pd.DataFrame()\n",
    "date_probs_df['date_prob_pctl'] = date_probs.apply(compare_percentile)\n",
    "date_probs_df = date_probs_df.reset_index()\n",
    "date_probs_df['date'] = pd.to_datetime(date_probs_df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_date_probs(df, date_probs):\n",
    "    df = pd.merge(df, date_probs, \n",
    "                  how='left', on='date')\n",
    "    return df\n",
    "\n",
    "train_raw = add_date_probs(train_raw, date_probs_df)\n",
    "test_raw = add_date_probs(test_raw, date_probs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add activity history columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_history_cols(df, activity_history):\n",
    "    df = pd.merge(df, activity_history,\n",
    "                 how='left', on=['people_id','date'])\n",
    "    return df\n",
    "\n",
    "train_raw = add_history_cols(train_raw, activity_history)\n",
    "test_raw = add_history_cols(test_raw, activity_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reindex and clear memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_raw.index = train_raw['people_id']\n",
    "test_raw.index = test_raw['people_id']\n",
    "people.index = people['people_id']\n",
    "del train_raw['people_id']\n",
    "del test_raw['people_id']\n",
    "del people['people_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# updated people cols\n",
    "people.columns = map(lambda x: 'p_'+x if x!='date' else x, people.columns)\n",
    "people = update_cols(people, 0)\n",
    "\n",
    "# train features\n",
    "train_raw = update_cols(train_raw, 1)\n",
    "\n",
    "# test features\n",
    "test_raw = update_cols(test_raw, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transform data to sparse OHE matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# merge people and training/test data \n",
    "train_full = pd.merge(train_raw, people, left_index=True, right_index=True)\n",
    "test_full = pd.merge(test_raw, people, left_index=True, right_index=True)\n",
    "\n",
    "# reset index to include people id\n",
    "# don't include people id because test set is split by people \n",
    "# train_full = train_full.reset_index(level=0)\n",
    "# test_full = test_full.reset_index(level=0)\n",
    "\n",
    "# reindex with activity_id and remove as feature\n",
    "train_full.index = train_full['activity_id']\n",
    "test_full.index = test_full['activity_id']\n",
    "del train_full['activity_id']\n",
    "del test_full['activity_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1459765583276749"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train_full.memory_usage())*1.0/1024**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26008553802967072"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(test_full.memory_usage())*1.0/1024**3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create OHE dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_values = set(train_full.as_matrix().reshape(1,-1).ravel())\n",
    "ohe_dict= { k: v for k,v in zip(all_values, xrange(len(all_values))) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36871"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ohe_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create sparse matrices for test and training datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = create_sparse_ohe_data(train_full, ohe_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test = create_sparse_ohe_data(test_full, ohe_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IDs = np.array(test_full.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2197291, 36871)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clear unused memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del train_full \n",
    "del train_raw\n",
    "del test_full\n",
    "del test_raw\n",
    "del people \n",
    "del ohe_dict\n",
    "del all_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dump_files(X, X_test, Y, IDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not check_exists('X') or check_exists('Y'):\n",
    "    read_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model\n",
    "Use frequency of outcome variable for prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-loss: 0.686851739242 Accuracy: 0.556045603427 AUC:0.5\n"
     ]
    }
   ],
   "source": [
    "p = Y.value_counts()*1.0 / X.shape[0]\n",
    "\n",
    "N = X.shape[0]\n",
    "predictions = np.zeros((N,2))\n",
    "for i in xrange(N):\n",
    "    predictions[i,:] = p\n",
    "\n",
    "a = accuracy_score(Y, np.argmax(predictions,axis=1))\n",
    "ll = log_loss(Y, predictions)\n",
    "auc = roc_auc_score(np.c_[Y==0,Y==1], predictions)\n",
    "print 'Log-loss: {ll} Accuracy: {acc} AUC:{auc}'.format(acc=a,ll=ll,auc=auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "Would expect improvement from the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-Loss:2.2389076 Accuracy:0.8650955 AUC:0.9172802\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# split test and train\n",
    "N = X.shape[0]\n",
    "e = N/10*8\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.2)\n",
    "X_train, X_val, y_train, y_val = X[:e,:], X[e:,:], Y[:e], Y[e:]\n",
    "\n",
    "# train model \n",
    "nb = BernoulliNB(alpha=1e-12, binarize=0.0)\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# make predictions \n",
    "predictions = nb.predict_proba(X_val)\n",
    "_ = test_scores(y_val, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svc = LinearSVC(C=1.0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Accuracy: 0.941600922953\n"
     ]
    }
   ],
   "source": [
    "a = np.sum(svc.predict(X_val) == y_val)*1.0 / y_val.shape[0]\n",
    "print 'Raw Accuracy: {}'.format(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prob_pos = svc.decision_function(X_val)\n",
    "prob_pos = (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())\n",
    "predictions = np.zeros((X_val.shape[0],2))\n",
    "predictions[:,1] = prob_pos\n",
    "predictions[:,0] = 1-prob_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-Loss:0.3917738 Accuracy:0.9004617 AUC:0.9840613\n"
     ]
    }
   ],
   "source": [
    "_ = test_scores(y_val, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_scores_xgb(y_val, predictions):\n",
    "    pfull = np.c_[1-predictions,predictions]\n",
    "    ll = log_loss(y_val, pfull)\n",
    "    a = accuracy_score(y_val, np.argmax(pfull ,axis=1))\n",
    "    auc = roc_auc_score(np.c_[y_val==0, y_val==1], pfull)\n",
    "    r = {'logloss':ll,'accuracy':a,'AUC':auc}\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix( X_train, label=y_train.as_matrix() )\n",
    "dval = xgb.DMatrix( X_val, label=y_val.as_matrix() )\n",
    "dtest = xgb.DMatrix( X_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = 10\n",
    "e = 0.05\n",
    "param = {'max_depth':d, \n",
    "         'eta':e, \n",
    "         'subsample':1.0, \n",
    "         'colsample_bytree':0.25,\n",
    "         'silent':1, \n",
    "         'lambda':1.0,\n",
    "         'objective':'binary:logistic',\n",
    "#          'min_child_weight':0,\n",
    "#          'booster':'gblinear'\n",
    "        }\n",
    "param['eval_metric'] = 'auc'\n",
    "param['nthread'] = CPUS\n",
    "evallist  = [(dval,'eval'), (dtrain,'train')]\n",
    "xgb_model = xgb.train(param.items(), dtrain, 1001, evallist, verbose_eval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = xgb_model.predict( dval )\n",
    "test_scores_xgb(y_val, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check model parameter tuning\n",
    "Using a sample of data to limit runtime.  \n",
    "Assumes that sampled validation curve will match full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_charts = True\n",
    "if run_charts:\n",
    "    sample = np.random.choice(xrange(X.shape[0]),size=100000)\n",
    "    lr_validation_curve(X[sample,:], \n",
    "                        Y.iloc[sample], \n",
    "                        param_range= np.logspace(-3, 5, 6),\n",
    "                        n_jobs=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split train and val "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = X.shape[0]\n",
    "e = N/10*8\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.2)\n",
    "X_train, X_val, y_train, y_val = X[:e,:], X[e:,:], Y[:e], Y[e:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = 1e2\n",
    "lr = LogisticRegression(C=c, max_iter=200, tol=1e-4, n_jobs=CPUS)\n",
    "lr.fit(X_train, y_train)\n",
    "predictions = lr.predict_proba(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-Loss:0.1819167 Accuracy:0.9399489 AUC:0.9842792\n"
     ]
    }
   ],
   "source": [
    "_ = test_scores(y_val, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Logistic Regression Result Tracker</h3>\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "    <th>Iter</th>\n",
    "    <th>C</th>\n",
    "    <th>Clusters</th>\n",
    "    <th>Tol</th>\n",
    "    <th>+Features</th>\n",
    "    <th>AUC</th>\n",
    "    <th>Log Loss</th>\n",
    "    <th>Accuracy</th>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>200</td>\n",
    "    <td>100</td>\n",
    "    <td>1</td>\n",
    "    <td>1e-5</td>\n",
    "    <td></td>\n",
    "    <td>0.9767696</td>\n",
    "    <td>0.2064201</td>\n",
    "    <td>0.9075454</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>200</td>\n",
    "    <td>100</td>\n",
    "    <td>1</td>\n",
    "    <td>1e-5</td>\n",
    "    <td>+Activity history</td>\n",
    "    <td>0.9767806</td>\n",
    "    <td>0.2046148</td>\n",
    "    <td>0.9075249</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>200</td>\n",
    "    <td>100</td>\n",
    "    <td>1</td>\n",
    "    <td>1e-5</td>\n",
    "    <td>+Date Prob</td>\n",
    "    <td>0.9768128</td>\n",
    "    <td>0.2063661</td>\n",
    "    <td>0.9080028</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>300</td>\n",
    "    <td>100</td>\n",
    "    <td>1</td>\n",
    "    <td>1e-5</td>\n",
    "    <td>+More accurate history</td>\n",
    "    <td>0.9768163</td>\n",
    "    <td>0.2062309</td>\n",
    "    <td>0.9082030</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>200</td>\n",
    "    <td>100</td>\n",
    "    <td>1</td>\n",
    "    <td>1e-4</td>\n",
    "    <td>-Iterations and tol</td>\n",
    "    <td>0.9770576</td>\n",
    "    <td>0.2029024</td>\n",
    "    <td>0.9088720</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>200</td>\n",
    "    <td>100</td>\n",
    "    <td>1</td>\n",
    "    <td>1e-3</td>\n",
    "    <td>-more tol</td>\n",
    "    <td>0.9766761</td>\n",
    "    <td>0.1920801</td>\n",
    "    <td>0.9106515</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>200</td>\n",
    "    <td>100</td>\n",
    "    <td>1</td>\n",
    "    <td>1e-4</td>\n",
    "    <td>+SVM prediction</td>\n",
    "    <td>0.9842792</td>\n",
    "    <td>0.1819167</td>\n",
    "    <td>0.9399489</td>\n",
    "</tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to filter out non-useful coefficients\n",
    "F = (0.15*lr.coef_.shape[1])\n",
    "A = np.argsort(np.abs(lr.coef_))[:,::-1].ravel()[:-F]\n",
    "# X = X[:,A]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c     = 1e2\n",
    "tol   = 1e-4\n",
    "iters = 200\n",
    "lr = LogisticRegression(C=c, max_iter=iters, tol=tol, n_jobs=CPUS)\n",
    "lr.fit(X, Y)\n",
    "predictions = lr.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(np.vstack((IDs, \n",
    "                                     predictions[:,1])).T,\n",
    "                          columns=['activity_id','outcome'])\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use already-known predictions via forums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_leak_data(submission, loc='./Data/leak.csv.gz'):\n",
    "    leak = pd.read_csv(loc)\n",
    "    leak.columns = ['activity_id','leak_outcome']\n",
    "    leak = leak.loc[(leak['leak_outcome']==1.0) | (leak['leak_outcome']==0.0),:]\n",
    "    leak_submission = pd.merge(submission, leak, how='left', on='activity_id')\n",
    "    nonleak = np.isnan(leak_submission['leak_outcome'])\n",
    "    leak_submission.loc[nonleak,'leak_outcome'] = leak_submission.loc[nonleak, 'outcome']\n",
    "    del leak_submission['outcome']\n",
    "    leak_submission.columns = ['activity_id','outcome']\n",
    "    return leak_submission\n",
    "\n",
    "leak_submission = add_leak_data(submission)\n",
    "leak_submission.to_csv('leak_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(criterion='entropy',  max_depth=3)\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree.export_graphviz(dt,\n",
    "     out_file='tree.dot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!dot -Tpng tree.dot -o tree.png "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Decision tree </h3>\n",
    "<img src='tree.png' align='left' height='100%' width='500px'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# all thresholds are 0.5\n",
    "threshold = 0.5\n",
    "\n",
    "# first level\n",
    "feature1 = 11566\n",
    "f1_l = (X_train[:,feature1]<=threshold).A.ravel()\n",
    "f1_r = (X_train[:,feature1]>threshold).A.ravel()\n",
    "\n",
    "# second level\n",
    "feature2 = 19318\n",
    "f2_l = f1_l & (X_train[:,feature2]<=threshold).A.ravel()\n",
    "f2_r = f1_r & (X_train[:,feature2]>threshold).A.ravel()\n",
    "\n",
    "# third level \n",
    "feature3 = 6755\n",
    "f3_l = f2_l & (X_train[:,feature3]<=threshold).A.ravel()\n",
    "f3_r = f2_l & (X_train[:,feature3]>threshold).A.ravel()\n",
    "\n",
    "# models\n",
    "model_f1_r = [ 1.0, 0.0 ] # val outcome is always zero \n",
    "model_f2_r = [ 1.0, 0.0 ] # val outcome is nearly always zero \n",
    "model_f3_l = LogisticRegression(C=0.1, max_iter=200, tol=1e-4).fit(X_train[f3_l],y_train[f3_l])\n",
    "model_f3_r = LogisticRegression(C=0.1, max_iter=200, tol=1e-4).fit(X_train[f3_r],y_train[f3_r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first level\n",
    "f1_l = (X_val[:,feature1]<=threshold).A.ravel()\n",
    "f1_r = (X_val[:,feature1]>threshold).A.ravel()\n",
    "\n",
    "# second level\n",
    "f2_l = f1_l & (X_val[:,feature2]<=threshold).A.ravel()\n",
    "f2_r = f1_r & (X_val[:,feature2]>threshold).A.ravel()\n",
    "\n",
    "# third level \n",
    "f3_l = f2_l & (X_val[:,feature3]<=threshold).A.ravel()\n",
    "f3_r = f2_l & (X_val[:,feature3]>threshold).A.ravel()\n",
    "\n",
    "# predict\n",
    "predictions = np.zeros((y_val.shape[0], 2))\n",
    "predictions[f1_r] = model_f1_r\n",
    "predictions[f2_r] = model_f2_r\n",
    "predictions[f3_l] = model_f3_l.predict_proba(X_val[f3_l])\n",
    "predictions[f3_r] = model_f3_r.predict_proba(X_val[f3_r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = test_scores(y_val, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>DT Logistic Regression Result Tracker</h3>\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "    <th>Iter</th>\n",
    "    <th>C</th>\n",
    "    <th>Tol</th>\n",
    "    <th>Depth</th>\n",
    "    <th>AUC</th>\n",
    "    <th>Log Loss</th>\n",
    "    <th>Accuracy</th>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>200</td>\n",
    "    <td>1.0</td>\n",
    "    <td>1e-4</td>\n",
    "    <td>1</td>\n",
    "    <td>0.9744524</td>\n",
    "    <td>0.1952951</td>\n",
    "    <td>0.9077684</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>200</td>\n",
    "    <td>10.0</td>\n",
    "    <td>1e-4</td>\n",
    "    <td>1</td>\n",
    "    <td>0.9771368</td>\n",
    "    <td>0.1897558</td>\n",
    "    <td>0.9108609</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>200</td>\n",
    "    <td>10.0</td>\n",
    "    <td>1e-4</td>\n",
    "    <td>2</td>\n",
    "    <td>0.9775437</td>\n",
    "    <td>0.1890088</td>\n",
    "    <td>0.9107107</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>200</td>\n",
    "    <td>100.0</td>\n",
    "    <td>1e-4</td>\n",
    "    <td>3</td>\n",
    "    <td>0.9630686</td>\n",
    "    <td>0.2355023</td>\n",
    "    <td>0.8928751</td>\n",
    "</tr>\n",
    "\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
