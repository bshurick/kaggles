{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting getfiles.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile getfiles.sh\n",
    "!/usr/bin/env bash\n",
    "\n",
    "sudo mkfs -t ext4 /dev/xvdb\n",
    "sudo mount /dev/xvdb /Data\n",
    "sudo chown -R ubuntu:ubuntu /Data\n",
    "mkdir /Data/tmp\n",
    "mkdir /Data/tmp/logs\n",
    "mkdir /Data/tmp/jobs\n",
    "\n",
    "files=(https://www.dropbox.com/s/ikqguxu91yf8fiz/train_numeric.csv.gz\\\n",
    "        https://www.dropbox.com/s/9kl3m7ssx7uhnda/test_numeric.csv.gz\\\n",
    "        https://www.dropbox.com/s/59vl5rch97v7t6c/test_date.csv.gz\\\n",
    "        https://www.dropbox.com/s/anxa8zqei68uidp/train_date.csv.gz\\\n",
    "        https://www.dropbox.com/s/y6syumln123x4lb/train_categorical.csv.gz\\\n",
    "        https://www.dropbox.com/s/eczq1y1uzzy66el/test_categorical.csv.gz)\n",
    "\n",
    "cd /Data\n",
    "for f in ${files[@]}\n",
    "do\n",
    "    wget $f\n",
    "done\n",
    "\n",
    "files=(train_numeric test_numeric test_date train_date train_categorical test_categorical)\n",
    "for f in ${files[@]}\n",
    "do\n",
    "    cat $f.csv.gz | gzip -d | head -n 10000 | gzip > testfile_$f.csv.gz\n",
    "    mkdir /Data/$f\n",
    "    cp $f.csv.gz $f\n",
    "    cd /Data/$f\n",
    "    gzip -d $f.csv.gz\n",
    "    split -l 50000 $f.csv fp_\n",
    "    rm $f.csv\n",
    "    cd /Data\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting emr_getheaders.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile emr_getheaders.sh\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "rm *.csv.gz\n",
    "files=(https://www.dropbox.com/s/ikqguxu91yf8fiz/train_numeric.csv.gz\\\n",
    "        https://www.dropbox.com/s/9kl3m7ssx7uhnda/test_numeric.csv.gz\\\n",
    "        https://www.dropbox.com/s/59vl5rch97v7t6c/test_date.csv.gz\\\n",
    "        https://www.dropbox.com/s/anxa8zqei68uidp/train_date.csv.gz\\\n",
    "        https://www.dropbox.com/s/y6syumln123x4lb/train_categorical.csv.gz\\\n",
    "        https://www.dropbox.com/s/eczq1y1uzzy66el/test_categorical.csv.gz)\n",
    "for f in ${files[@]}\n",
    "do\n",
    "    wget $f\n",
    "done\n",
    "\n",
    "files=(train_numeric test_numeric train_date test_date train_categorical test_categorical)\n",
    "for f in ${files[@]}\n",
    "do\n",
    "    cat $f.csv.gz | gzip -d | head -n 1 > $f.csv\n",
    "    rm $f.csv.gz\n",
    "    gzip $f.csv\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing emr_getfiles.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile emr_getfiles.sh\n",
    "#!/usr/bin/env bash\n",
    "nn1=$1\n",
    "\n",
    "hadoop distcp s3a://brandonshurick/Data/train_categorical hdfs://$nn1/user/hadoop/train_categorical\n",
    "hadoop distcp s3a://brandonshurick/Data/train_numeric hdfs://$nn1/user/hadoop/train_numeric\n",
    "hadoop distcp s3a://brandonshurick/Data/train_date hdfs://$nn1/user/hadoop/train_date\n",
    "\n",
    "hadoop distcp s3a://brandonshurick/Data/test_categorical hdfs://$nn1/user/hadoop/test_categorical\n",
    "hadoop distcp s3a://brandonshurick/Data/test_numeric hdfs://$nn1/user/hadoop/test_numeric\n",
    "hadoop distcp s3a://brandonshurick/Data/test_date hdfs://$nn1/user/hadoop/test_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple, defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hashlib\n",
    "from math import log, exp, sqrt\n",
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# for viewing data samples\n",
    "pd.options.display.max_rows = 5000\n",
    "\n",
    "# base directory\n",
    "DIR = 'hdfs:///user/hadoop/'\n",
    "SUFFIX = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hashFunction(numBuckets, rawFeats, printMapping=False):\n",
    "    \"\"\"Calculate a feature dictionary for an observation's features based on hashing.\n",
    "\n",
    "    Note:\n",
    "        Use printMapping=True for debug purposes and to better understand how the hashing works.\n",
    "\n",
    "    Args:\n",
    "        numBuckets (int): Number of buckets to use as features.\n",
    "        rawFeats (list of (int, str)): A list of features for an observation.  Represented as\n",
    "            (featureID, value) tuples.\n",
    "        printMapping (bool, optional): If true, the mappings of featureString to index will be\n",
    "            printed.\n",
    "\n",
    "    Returns:\n",
    "        dict of int to float:  The keys will be integers which represent the buckets that the\n",
    "            features have been hashed to.  The value for a given key will contain the count of the\n",
    "            (featureID, value) tuples that have hashed to that key.\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    for ind, category in rawFeats:\n",
    "        featureString = category + str(ind)\n",
    "        mapping[featureString] = int(int(hashlib.md5(featureString).hexdigest(), 16) % numBuckets)\n",
    "    if(printMapping): print mapping\n",
    "    sparseFeatures = defaultdict(float)\n",
    "    for bucket in mapping.values():\n",
    "        sparseFeatures[bucket] += 1.0\n",
    "    return dict(sparseFeatures)\n",
    "\n",
    "\n",
    "def parseHashPoint(point, numBuckets):\n",
    "    \"\"\"Create a LabeledPoint for this observation using hashing.\n",
    "\n",
    "    Args:\n",
    "        point (str): A comma separated string where the first value is the label and the rest are\n",
    "            features.\n",
    "        numBuckets: The number of buckets to hash to.\n",
    "\n",
    "    Returns:\n",
    "        LabeledPoint: A LabeledPoint with a label (0.0 or 1.0) and a SparseVector of hashed\n",
    "            features.\n",
    "    \"\"\"\n",
    "    splits = point.split(',')\n",
    "    fields = [ (i,v) for i,v in enumerate(splits[1:]) ]\n",
    "    vec = SparseVector(numBuckets, hashFunction(numBuckets, fields))\n",
    "    return LabeledPoint(splits[0], vec)\n",
    "\n",
    "\n",
    "def getP(x, w, intercept):\n",
    "    \"\"\"Calculate the probability for an observation given a set of weights and intercept.\n",
    "\n",
    "    Note:\n",
    "        We'll bound our raw prediction between 20 and -20 for numerical purposes.\n",
    "\n",
    "    Args:\n",
    "        x (SparseVector): A vector with values of 1.0 for features that exist in this\n",
    "            observation and 0.0 otherwise.\n",
    "        w (DenseVector): A vector of weights (betas) for the model.\n",
    "        intercept (float): The model's intercept.\n",
    "\n",
    "    Returns:\n",
    "        float: A probability between 0 and 1.\n",
    "    \"\"\"\n",
    "    rawPrediction = x.dot(w)+intercept\n",
    "\n",
    "    # Bound the raw prediction value\n",
    "    rawPrediction = min(rawPrediction, 20)\n",
    "    rawPrediction = max(rawPrediction, -20)\n",
    "    return 1.0/(1+exp(-rawPrediction))\n",
    "\n",
    "\n",
    "def computeLogLoss(p, y):\n",
    "    \"\"\"Calculates the value of log loss for a given probabilty and label.\n",
    "\n",
    "    Note:\n",
    "        log(0) is undefined, so when p is 0 we need to add a small value (epsilon) to it\n",
    "        and when p is 1 we need to subtract a small value (epsilon) from it.\n",
    "\n",
    "    Args:\n",
    "        p (float): A probabilty between 0 and 1.\n",
    "        y (int): A label.  Takes on the values 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "        float: The log loss value.\n",
    "    \"\"\"\n",
    "    epsilon = 10e-12\n",
    "    if p==0:\n",
    "        p+=epsilon\n",
    "    elif p==1:\n",
    "        p-=epsilon\n",
    "    if y==1:\n",
    "        return -log(p)\n",
    "    elif y==0:\n",
    "        return -log(1-p)\n",
    "    else:\n",
    "        raise Exception('y not in {0,1}')\n",
    "\n",
    "\n",
    "def getP(x, w, intercept):\n",
    "    \"\"\"Calculate the probability for an observation given a set of weights and intercept.\n",
    "\n",
    "    Note:\n",
    "        We'll bound our raw prediction between 20 and -20 for numerical purposes.\n",
    "\n",
    "    Args:\n",
    "        x (SparseVector): A vector with values of 1.0 for features that exist in this\n",
    "            observation and 0.0 otherwise.\n",
    "        w (DenseVector): A vector of weights (betas) for the model.\n",
    "        intercept (float): The model's intercept.\n",
    "\n",
    "    Returns:\n",
    "        float: A probability between 0 and 1.\n",
    "    \"\"\"\n",
    "    rawPrediction = x.dot(w)+intercept\n",
    "\n",
    "    # Bound the raw prediction value\n",
    "    rawPrediction = min(rawPrediction, 20)\n",
    "    rawPrediction = max(rawPrediction, -20)\n",
    "    return 1.0/(1+exp(-rawPrediction))\n",
    "\n",
    "\n",
    "def getMccData(p, x):\n",
    "    tp, tn, fp, fn = [0]*4\n",
    "    v = int((p>0.5)*1)\n",
    "    a = int(x)\n",
    "    if v==1 and v==a:\n",
    "        tp = 1\n",
    "    elif v==0 and v==a:\n",
    "        tn = 1\n",
    "    elif v==1 and v!=a:\n",
    "        fp = 1\n",
    "    elif v==0 and v!=a:\n",
    "        fn = 1\n",
    "    return tp, tn, fp, fn\n",
    "\n",
    "\n",
    "def getMcc(data_px):\n",
    "    mccdata = data_px.map(lambda x: getMccData(x[0],x[1]))\n",
    "    tp = mccdata.map(lambda x: x[0]).sum()\n",
    "    tn = mccdata.map(lambda x: x[1]).sum()\n",
    "    fp = mccdata.map(lambda x: x[2]).sum()\n",
    "    fn = mccdata.map(lambda x: x[3]).sum()\n",
    "    mcc_num = (tp*tn) - (fp*fn)\n",
    "    mcc_base = sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "    try:\n",
    "        return mcc_num*1.0/mcc_base\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def print_scores(model):\n",
    "    print 'Model log-loss: {}'.format(model.scores.logloss)\n",
    "    print 'Model accuracy: {}'.format(model.scores.accuracy)\n",
    "    print 'Model MCC: {}'.format(model.scores.mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# named tuples for code readability \n",
    "Files = namedtuple('filelist',['categorical','date','numeric'])\n",
    "Header = namedtuple('header',['categorical','date','numeric'])\n",
    "Data = namedtuple('data',['categorical','date','numeric','outcome'])\n",
    "Model = namedtuple('model',['model','scores'])\n",
    "Scores = namedtuple('scores',['logloss','accuracy','mcc'])\n",
    "\n",
    "# declare file locations \n",
    "train_files = Files(DIR+'train_categorical'+SUFFIX,\n",
    "                DIR+'train_date'+SUFFIX,\n",
    "                DIR+'train_numeric'+SUFFIX)\n",
    "test_files = Files(DIR+'test_categorical'+SUFFIX,\n",
    "                DIR+'test_date'+SUFFIX,\n",
    "                DIR+'test_numeric'+SUFFIX)\n",
    "train_files_header = Files('train_categorical.csv.gz',\n",
    "                'train_date.csv.gz',\n",
    "                'train_numeric.csv.gz')\n",
    "test_files_header = Files('test_categorical.csv.gz',\n",
    "                'test_date.csv.gz',\n",
    "                'test_numeric.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cache raw data \n",
    "train_raw = Data(sc.textFile(train_files.categorical).cache(),\n",
    "                 sc.textFile(train_files.date).cache(),\n",
    "                 sc.textFile(train_files.numeric).cache(),None)\n",
    "test_raw = Data(sc.textFile(test_files.categorical).cache(),\n",
    "                sc.textFile(test_files.date).cache(),\n",
    "                sc.textFile(test_files.numeric).cache(),None)\n",
    "\n",
    "# headers \n",
    "get_header = lambda x: list(pd.read_csv(x,nrows=0).columns.values)\n",
    "train_headers = Header(get_header(train_files_header.categorical),\n",
    "                       get_header(train_files_header.date),\n",
    "                       get_header(train_files_header.numeric))\n",
    "test_headers = Header(get_header(test_files_header.categorical),\n",
    "                       get_header(test_files_header.date),\n",
    "                       get_header(test_files_header.numeric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# filter out first row\n",
    "remove_header = lambda x: x.split(',')[0]!='Id'\n",
    "\n",
    "\n",
    "# return outcome from numeric data\n",
    "get_outcome = lambda (k,v): (k, int(v[-1]))\n",
    "subtract_outcome = lambda (k,v): (k, v[:-1])\n",
    "\n",
    "\n",
    "# split key and fields\n",
    "id_split = lambda x: (int(x.split(',')[0]),x.split(',')[1:])\n",
    "def header_key(x, headers):\n",
    "    ''' split id from fields\n",
    "        emit header as key\n",
    "    '''\n",
    "    id_int = x[0]\n",
    "    fields = x[1]\n",
    "    for i, f in enumerate(fields):\n",
    "        h = headers[1:][i]\n",
    "        yield (h, [(id_int, f)])\n",
    "\n",
    "\n",
    "# gather numeric features\n",
    "def convert_numeric(x):\n",
    "    ''' Gather numeric features '''\n",
    "    k,v = x\n",
    "    num = float(v[0][1]) if len(v[0][1])>0 else 0\n",
    "    return (k, [(v[0][0], num)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter header from data and get outcome\n",
    "train_filtered = Data(\n",
    "    train_raw.categorical.filter(remove_header).map(id_split),\n",
    "    train_raw.date.filter(remove_header).map(id_split),\n",
    "    train_raw.numeric.filter(remove_header).map(id_split),\n",
    "    train_raw.numeric.filter(remove_header).map(id_split).map(get_outcome)\n",
    ")\n",
    "test_filtered = Data(\n",
    "    test_raw.categorical.filter(remove_header).map(id_split),\n",
    "    test_raw.date.filter(remove_header).map(id_split),\n",
    "    test_raw.numeric.filter(remove_header).map(id_split),\n",
    "    None #unknown\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# group data by column \n",
    "train_explode = Data(\n",
    "    train_filtered.categorical.flatMap(lambda x: header_key(x, train_headers.categorical)),\n",
    "    train_filtered.date.flatMap(lambda x: header_key(x, train_headers.date)),\n",
    "    train_filtered.numeric.flatMap(lambda x: header_key(x, train_headers.numeric)).map(convert_numeric),\n",
    "    train_filtered.outcome\n",
    ")\n",
    "test_explode = Data(\n",
    "    test_filtered.categorical.flatMap(lambda x: header_key(x, train_headers.categorical)),\n",
    "    test_filtered.date.flatMap(lambda x: header_key(x, train_headers.date)),\n",
    "    test_filtered.numeric.flatMap(lambda x: header_key(x, train_headers.numeric)).map(convert_numeric),\n",
    "    None #unknown\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create key-val RDDs\n",
    "train_rdd = Data(\n",
    "    train_filtered.categorical,\n",
    "    train_filtered.date,\n",
    "    train_filtered.numeric.map(subtract_outcome),\n",
    "    train_filtered.outcome\n",
    ")\n",
    "test_rdd = Data(\n",
    "    test_filtered.categorical,\n",
    "    test_filtered.date,\n",
    "    test_filtered.numeric,\n",
    "    None #unknown\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# explore categorical columns\n",
    "def groupby_col(x):\n",
    "    k, v= x\n",
    "    colval = v[0][1]\n",
    "    return ((k,colval),1)\n",
    "col_counts = train_explode.categorical.filter(lambda (k,x): len(x[0][1])>0).\\\n",
    "                          map(groupby_col).reduceByKey(lambda x,y: x+y).\\\n",
    "                          collect()\n",
    "col_counts.sort(key=lambda x: x[0])\n",
    "total_count = train_explode.categorical.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col_counts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# explore numeric columns\n",
    "def groupby_col(x):\n",
    "    k, v= x\n",
    "    colval = v[0][1]\n",
    "    return (k, colval)\n",
    "col_sums = train_explode.numeric.map(groupby_col).reduceByKey(lambda x,y: x+y)\n",
    "col_totals = train_explode.numeric.map(lambda (k,v): (k,1)).reduceByKey(lambda x,y: x+y)\n",
    "col_totals = col_sums.join(col_totals).collect()\n",
    "col_means = [ (x, y[0]/y[1]) for x,y in col_totals ]\n",
    "col_means.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col_means[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# look at first field\n",
    "pd.DataFrame(zip(train_headers.numeric[1:], \n",
    "                 train_rdd.numeric.first()[1]),\n",
    "             columns=['Field','NumVal']).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate baseline model\n",
    "sum_response = train_explode.numeric.filter(lambda x: x[0]=='Response').\\\n",
    "                                     map(lambda x: x[1][0][1]).\\\n",
    "                                     reduce(lambda x,y: x+y)\n",
    "count_response = train_explode.numeric.filter(lambda x: x[0]=='Response').count()\n",
    "baseline = sum_response*1.0/count_response\n",
    "logloss = train_filtered.outcome.map(lambda x: computeLogLoss(baseline, x[1])).sum() / train_filtered.outcome.count()\n",
    "accuracy = 1-baseline\n",
    "mcc = getMcc(train_filtered.outcome.map(lambda x: (baseline, x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Baseline log-loss: {}'.format(logloss)\n",
    "print 'Baseline accuracy: {}'.format(accuracy)\n",
    "print 'Baseline MCC: {}'.format(mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "def convertPointNumeric(x):\n",
    "    k, v = x\n",
    "    v1, response = v\n",
    "    features = []\n",
    "    for v_i in v1:\n",
    "        if len(v_i)>0:\n",
    "            features.append(float(v_i))\n",
    "        else:\n",
    "            features.append(0)\n",
    "    return LabeledPoint(response, features)\n",
    "    \n",
    "data = train_rdd.numeric.join(train_rdd.outcome).map(convertPointNumeric)\n",
    "training, val = data.randomSplit([0.7, 0.3])\n",
    "train_params = {\n",
    "    'iterations':300, \n",
    "    'initialWeights':[1]*len(training.first().features), \n",
    "    'regParam':0.001, \n",
    "    'regType':'l2', \n",
    "    'intercept':True, \n",
    "    'validateData':False, \n",
    "    'corrections':10, \n",
    "    'tolerance':1e-06, \n",
    "    'numClasses':2\n",
    "}\n",
    "m = LogisticRegressionWithLBFGS().train(training, **train_params)\n",
    "p = val.map(lambda x: (getP(x.features, m.weights, m.intercept), x.label))\n",
    "s = Scores(\n",
    "    p.map(lambda x: computeLogLoss(x[0], x[1])).sum() / p.count(),\n",
    "    p.map(lambda x: int(x[0]>0.5)==int(x[1])).sum()*1.0 / p.count(),\n",
    "    getMcc(p)\n",
    ")\n",
    "model1 = Model(m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_scores(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import GradientBoostedTrees\n",
    "data = train_rdd.numeric.join(train_rdd.outcome).map(convertPointNumeric)\n",
    "training, val = data.randomSplit([0.7, 0.3])\n",
    "gb_params = {\n",
    "    'categoricalFeaturesInfo':{},\n",
    "    'numIterations':300,\n",
    "    'loss':'leastSquaresError', #logLoss\n",
    "    'learningRate':0.1, \n",
    "    'maxDepth':3, \n",
    "    'maxBins':32,\n",
    "}\n",
    "m = GradientBoostedTrees.trainClassifier(training, **gb_params)\n",
    "p = m.predict(val.map(lambda x: x.features))\n",
    "lp = val.map(lambda lp: lp.label).zip(p).map(lambda x: (x[1],x[0]))\n",
    "s = Scores(\n",
    "    lp.map(lambda x: computeLogLoss(x[0], x[1])).sum() / lp.count(),\n",
    "    lp.map(lambda x: int(x[0]>0.5)==int(x[1])).sum()*1.0 / lp.count(),\n",
    "    getMcc(lp)\n",
    ")\n",
    "model2 = Model(m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_scores(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
