{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple, defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hashlib\n",
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# for viewing data samples\n",
    "pd.options.display.max_rows = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hashFunction(numBuckets, rawFeats, printMapping=False):\n",
    "    \"\"\"Calculate a feature dictionary for an observation's features based on hashing.\n",
    "\n",
    "    Note:\n",
    "        Use printMapping=True for debug purposes and to better understand how the hashing works.\n",
    "\n",
    "    Args:\n",
    "        numBuckets (int): Number of buckets to use as features.\n",
    "        rawFeats (list of (int, str)): A list of features for an observation.  Represented as\n",
    "            (featureID, value) tuples.\n",
    "        printMapping (bool, optional): If true, the mappings of featureString to index will be\n",
    "            printed.\n",
    "\n",
    "    Returns:\n",
    "        dict of int to float:  The keys will be integers which represent the buckets that the\n",
    "            features have been hashed to.  The value for a given key will contain the count of the\n",
    "            (featureID, value) tuples that have hashed to that key.\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    for ind, category in rawFeats:\n",
    "        featureString = category + str(ind)\n",
    "        mapping[featureString] = int(int(hashlib.md5(featureString).hexdigest(), 16) % numBuckets)\n",
    "    if(printMapping): print mapping\n",
    "    sparseFeatures = defaultdict(float)\n",
    "    for bucket in mapping.values():\n",
    "        sparseFeatures[bucket] += 1.0\n",
    "    return dict(sparseFeatures)\n",
    "\n",
    "\n",
    "def parseHashPoint(point, numBuckets):\n",
    "    \"\"\"Create a LabeledPoint for this observation using hashing.\n",
    "\n",
    "    Args:\n",
    "        point (str): A comma separated string where the first value is the label and the rest are\n",
    "            features.\n",
    "        numBuckets: The number of buckets to hash to.\n",
    "\n",
    "    Returns:\n",
    "        LabeledPoint: A LabeledPoint with a label (0.0 or 1.0) and a SparseVector of hashed\n",
    "            features.\n",
    "    \"\"\"\n",
    "    splits = point.split(',')\n",
    "    fields = [ (i,v) for i,v in enumerate(splits[1:]) ]\n",
    "    vec = SparseVector(numBuckets, hashFunction(numBuckets, fields))\n",
    "    return LabeledPoint(splits[0], vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# named tuples for code readability \n",
    "Files = namedtuple('filelist',['categorical','date','numeric'])\n",
    "Header = namedtuple('header',['categorical','date','numeric'])\n",
    "Data = namedtuple('data',['categorical','date','numeric','outcome'])\n",
    "\n",
    "# declare train file locations \n",
    "train_files = Files('./Data/train_categorical.csv.gz',\n",
    "                './Data/train_date.csv.gz',\n",
    "                './Data/train_numeric.csv.gz')\n",
    "test_files = Files('./Data/test_categorical.csv.gz',\n",
    "                './Data/test_date.csv.gz',\n",
    "                './Data/test_numeric.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cache raw data \n",
    "train_raw = Data(sc.textFile(train_files.categorical).cache(),\n",
    "                 sc.textFile(train_files.date).cache(),\n",
    "                 sc.textFile(train_files.numeric).cache(),None)\n",
    "test_raw = Data(sc.textFile(test_files.categorical).cache(),\n",
    "                sc.textFile(test_files.date).cache(),\n",
    "                sc.textFile(test_files.numeric).cache(),None)\n",
    "\n",
    "# headers \n",
    "get_header = lambda x: list(pd.read_csv(x,nrows=0).columns.values)\n",
    "train_headers = Header(get_header(train_files.categorical),\n",
    "                       get_header(train_files.date),\n",
    "                       get_header(train_files.numeric))\n",
    "test_headers = Header(get_header(test_files.categorical),\n",
    "                       get_header(test_files.date),\n",
    "                       get_header(test_files.numeric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter out first row\n",
    "remove_header = lambda x: x.split(',')[0]!='Id'\n",
    "\n",
    "\n",
    "# return outcome from numeric data\n",
    "get_outcome = lambda (k,v): (k, int(v[-1]))\n",
    "\n",
    "\n",
    "# split key and fields\n",
    "id_split = lambda x: (int(x.split(',')[0]),x.split(',')[1:])\n",
    "def key_split(x, headers):\n",
    "    ''' split id from fields\n",
    "        emit header \n",
    "    '''\n",
    "    id_int = int(x.split(',')[0])\n",
    "    fields = x.split(',')[1:]\n",
    "    for i, f in enumerate(fields):\n",
    "        h = headers[i]\n",
    "        yield (h, (id_int,[f]))\n",
    "\n",
    "\n",
    "# gather numeric features\n",
    "def convert_numeric(x):\n",
    "    ''' Gather numeric features '''\n",
    "    k,v = x\n",
    "    num = float(v[1][0]) if len(v[1][0])>0 else 0\n",
    "    return (k, (v[0], [num]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# filter header from data and get outcome\n",
    "train_filtered = Data(\n",
    "    train_raw.categorical.filter(remove_header),\n",
    "    train_raw.date.filter(remove_header),\n",
    "    train_raw.numeric.filter(remove_header).map(convert_numeric),\n",
    "    train_raw.numeric.filter(remove_header).map(id_split).map(get_outcome).repartition(100)\n",
    ")\n",
    "test_filtered = Data(\n",
    "    test_raw.categorical.filter(remove_header),\n",
    "    test_raw.date.filter(remove_header),\n",
    "    test_raw.numeric.filter(remove_header).map(convert_numeric),\n",
    "    test_raw.numeric.filter(remove_header).map(id_split).map(get_outcome).repartition(100)\n",
    ")\n",
    "\n",
    "\n",
    "# group data by column \n",
    "train_explode = Data(\n",
    "    train_filtered.categorical.flatMap(lambda x: key_split(x, train_headers.categorical)).repartition(100),\n",
    "    train_filtered.date.flatMap(lambda x: key_split(x, train_headers.date)).repartition(100),\n",
    "    train_filtered.numeric.flatMap(lambda x: key_split(x, train_headers.numeric)).map(convert_numeric).repartition(100),\n",
    "    train_filtered.outcome\n",
    ")\n",
    "test_explode = Data(\n",
    "    test_filtered.categorical.flatMap(lambda x: key_split(x, train_headers.categorical)).repartition(100),\n",
    "    test_filtered.date.flatMap(lambda x: key_split(x, train_headers.date)).repartition(100),\n",
    "    test_filtered.numeric.flatMap(lambda x: key_split(x, train_headers.numeric)).map(convert_numeric).repartition(100),\n",
    "    test_filtered.outcome\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_explode.categorical.reduceByKey(lambda x,y: x[1]+y[1]).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
