{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple, defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hashlib\n",
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# for viewing data samples\n",
    "pd.options.display.max_rows = 5000\n",
    "\n",
    "# base directory\n",
    "DIR = '/Data/testfile_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hashFunction(numBuckets, rawFeats, printMapping=False):\n",
    "    \"\"\"Calculate a feature dictionary for an observation's features based on hashing.\n",
    "\n",
    "    Note:\n",
    "        Use printMapping=True for debug purposes and to better understand how the hashing works.\n",
    "\n",
    "    Args:\n",
    "        numBuckets (int): Number of buckets to use as features.\n",
    "        rawFeats (list of (int, str)): A list of features for an observation.  Represented as\n",
    "            (featureID, value) tuples.\n",
    "        printMapping (bool, optional): If true, the mappings of featureString to index will be\n",
    "            printed.\n",
    "\n",
    "    Returns:\n",
    "        dict of int to float:  The keys will be integers which represent the buckets that the\n",
    "            features have been hashed to.  The value for a given key will contain the count of the\n",
    "            (featureID, value) tuples that have hashed to that key.\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    for ind, category in rawFeats:\n",
    "        featureString = category + str(ind)\n",
    "        mapping[featureString] = int(int(hashlib.md5(featureString).hexdigest(), 16) % numBuckets)\n",
    "    if(printMapping): print mapping\n",
    "    sparseFeatures = defaultdict(float)\n",
    "    for bucket in mapping.values():\n",
    "        sparseFeatures[bucket] += 1.0\n",
    "    return dict(sparseFeatures)\n",
    "\n",
    "\n",
    "def parseHashPoint(point, numBuckets):\n",
    "    \"\"\"Create a LabeledPoint for this observation using hashing.\n",
    "\n",
    "    Args:\n",
    "        point (str): A comma separated string where the first value is the label and the rest are\n",
    "            features.\n",
    "        numBuckets: The number of buckets to hash to.\n",
    "\n",
    "    Returns:\n",
    "        LabeledPoint: A LabeledPoint with a label (0.0 or 1.0) and a SparseVector of hashed\n",
    "            features.\n",
    "    \"\"\"\n",
    "    splits = point.split(',')\n",
    "    fields = [ (i,v) for i,v in enumerate(splits[1:]) ]\n",
    "    vec = SparseVector(numBuckets, hashFunction(numBuckets, fields))\n",
    "    return LabeledPoint(splits[0], vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# named tuples for code readability \n",
    "Files = namedtuple('filelist',['categorical','date','numeric'])\n",
    "Header = namedtuple('header',['categorical','date','numeric'])\n",
    "Data = namedtuple('data',['categorical','date','numeric','outcome'])\n",
    "\n",
    "# declare train file locations \n",
    "train_files = Files(DIR+'train_categorical.csv.gz',\n",
    "                DIR+'train_date.csv.gz',\n",
    "                DIR+'train_numeric.csv.gz')\n",
    "test_files = Files(DIR+'test_categorical.csv.gz',\n",
    "                DIR+'test_date.csv.gz',\n",
    "                DIR+'test_numeric.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cache raw data \n",
    "train_raw = Data(sc.textFile(train_files.categorical).cache(),\n",
    "                 sc.textFile(train_files.date).cache(),\n",
    "                 sc.textFile(train_files.numeric).cache(),None)\n",
    "test_raw = Data(sc.textFile(test_files.categorical).cache(),\n",
    "                sc.textFile(test_files.date).cache(),\n",
    "                sc.textFile(test_files.numeric).cache(),None)\n",
    "\n",
    "# headers \n",
    "get_header = lambda x: list(pd.read_csv(x,nrows=0).columns.values)\n",
    "train_headers = Header(get_header(train_files.categorical),\n",
    "                       get_header(train_files.date),\n",
    "                       get_header(train_files.numeric))\n",
    "test_headers = Header(get_header(test_files.categorical),\n",
    "                       get_header(test_files.date),\n",
    "                       get_header(test_files.numeric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# filter out first row\n",
    "remove_header = lambda x: x.split(',')[0]!='Id'\n",
    "\n",
    "\n",
    "# return outcome from numeric data\n",
    "get_outcome = lambda (k,v): (k, int(v[-1]))\n",
    "\n",
    "\n",
    "# split key and fields\n",
    "id_split = lambda x: (int(x.split(',')[0]),x.split(',')[1:])\n",
    "def header_key(x, headers):\n",
    "    ''' split id from fields\n",
    "        emit header as key\n",
    "    '''\n",
    "    id_int = int(x.split(',')[0])\n",
    "    fields = x.split(',')[1:]\n",
    "    for i, f in enumerate(fields):\n",
    "        h = headers[1:][i]\n",
    "        yield (h, [(id_int, f)])\n",
    "\n",
    "\n",
    "# gather numeric features\n",
    "def convert_numeric(x):\n",
    "    ''' Gather numeric features '''\n",
    "    k,v = x\n",
    "    num = float(v[0][1]) if len(v[0][1])>0 else 0\n",
    "    return (k, [(v[0][0], num)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter header from data and get outcome\n",
    "train_filtered = Data(\n",
    "    train_raw.categorical.filter(remove_header),\n",
    "    train_raw.date.filter(remove_header),\n",
    "    train_raw.numeric.filter(remove_header),\n",
    "    train_raw.numeric.filter(remove_header).map(id_split).map(get_outcome).repartition(100)\n",
    ")\n",
    "test_filtered = Data(\n",
    "    test_raw.categorical.filter(remove_header),\n",
    "    test_raw.date.filter(remove_header),\n",
    "    test_raw.numeric.filter(remove_header),\n",
    "    test_raw.numeric.filter(remove_header).map(id_split).map(get_outcome).repartition(100)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# group data by column \n",
    "train_explode = Data(\n",
    "    train_filtered.categorical.flatMap(lambda x: header_key(x, train_headers.categorical)).repartition(100),\n",
    "    train_filtered.date.flatMap(lambda x: header_key(x, train_headers.date)).repartition(100),\n",
    "    train_filtered.numeric.flatMap(lambda x: header_key(x, train_headers.numeric)).map(convert_numeric).repartition(100),\n",
    "    train_filtered.outcome\n",
    ")\n",
    "test_explode = Data(\n",
    "    test_filtered.categorical.flatMap(lambda x: header_key(x, train_headers.categorical)).repartition(100),\n",
    "    test_filtered.date.flatMap(lambda x: header_key(x, train_headers.date)).repartition(100),\n",
    "    test_filtered.numeric.flatMap(lambda x: header_key(x, train_headers.numeric)).map(convert_numeric).repartition(100),\n",
    "    test_filtered.outcome\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# explore categorical columns\n",
    "def groupby_col(x):\n",
    "    k, v= x\n",
    "    colval = v[0][1]\n",
    "    return ((k,colval),1)\n",
    "col_counts = train_explode.categorical.filter(lambda (k,x): len(x[0][1])>0).\\\n",
    "                          map(groupby_col).reduceByKey(lambda x,y: x+y).\\\n",
    "                          collect()\n",
    "col_counts.sort(key=lambda x: x[0][1])\n",
    "total_count = train_explode.categorical.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('L3_S32_F3854', u'T-21474819'), 1),\n",
       " (('L3_S32_F3854', u'T-2147482816'), 2),\n",
       " (('L3_S32_F3854', u'T-2147483648'), 4),\n",
       " (('L2_S26_F3104', u'T1'), 1856),\n",
       " (('L1_S24_F842', u'T1'), 90),\n",
       " (('L0_S22_F577', u'T1'), 1),\n",
       " (('L3_S29_F3429', u'T1'), 6606),\n",
       " (('L0_S21_F503', u'T1'), 1),\n",
       " (('L0_S9_F196', u'T1'), 3),\n",
       " (('L1_S25_F2865', u'T1'), 66)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_counts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21397860"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# explore numeric columns\n",
    "def groupby_col(x):\n",
    "    k, v= x\n",
    "    colval = v[0][1]\n",
    "    return (k, colval)\n",
    "col_sums = train_explode.numeric.map(groupby_col).reduceByKey(lambda x,y: x+y).collect()\n",
    "col_totals = train_explode.numeric.map(lambda (k,v): (k,1)).reduceByKey(lambda x,y: x+y).collect()\n",
    "col_sums.sort(); col_totals.sort()\n",
    "col_means = [ (x[0], x[1]/y[1]) for x,y in zip(col_sums, col_totals) if x[0]==y[0]]\n",
    "col_means.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('L0_S0_F0', -0.001087908790879088),\n",
       " ('L0_S0_F10', 0.0023501350135013543),\n",
       " ('L0_S0_F12', 0.00019881988198819863),\n",
       " ('L0_S0_F14', 0.0013297329732973305),\n",
       " ('L0_S0_F16', -0.0004527452745274525),\n",
       " ('L0_S0_F18', 0.0001511151115111509),\n",
       " ('L0_S0_F2', -0.0014882488248824884),\n",
       " ('L0_S0_F20', 0.0027981798179817986),\n",
       " ('L0_S0_F22', 0.0028061806180618063),\n",
       " ('L0_S0_F4', 0.0005809580958095808)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_means[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate baseline model\n",
    "sum_response = train_explode.numeric.filter(lambda x: x[0]=='Response').map(lambda x: x[1][0][1]).reduce(lambda x,y: x+y)\n",
    "count_response = train_explode.numeric.filter(lambda x: x[0]=='Response').count()\n",
    "baseline = [ 1-sum_response*1.0/count_response, sum_response*1.0/count_response ]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
