{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting getfiles.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile getfiles.sh\n",
    "!/usr/bin/env bash\n",
    "\n",
    "sudo mkfs -t ext4 /dev/xvdb\n",
    "sudo mount /dev/xvdb /Data\n",
    "sudo chown -R ubuntu:ubuntu /Data\n",
    "mkdir /Data/tmp\n",
    "mkdir /Data/tmp/logs\n",
    "mkdir /Data/tmp/jobs\n",
    "\n",
    "files=(https://www.dropbox.com/s/ikqguxu91yf8fiz/train_numeric.csv.gz\\\n",
    "        https://www.dropbox.com/s/9kl3m7ssx7uhnda/test_numeric.csv.gz\\\n",
    "        https://www.dropbox.com/s/59vl5rch97v7t6c/test_date.csv.gz\\\n",
    "        https://www.dropbox.com/s/anxa8zqei68uidp/train_date.csv.gz\\\n",
    "        https://www.dropbox.com/s/y6syumln123x4lb/train_categorical.csv.gz\\\n",
    "        https://www.dropbox.com/s/eczq1y1uzzy66el/test_categorical.csv.gz)\n",
    "\n",
    "cd /Data\n",
    "for f in ${files[@]}\n",
    "do\n",
    "    wget $f\n",
    "done\n",
    "\n",
    "files=(train_numeric test_numeric test_date train_date train_categorical test_categorical)\n",
    "for f in ${files[@]}\n",
    "do\n",
    "    cat $f.csv.gz | gzip -d | head -n 10000 | gzip > testfile_$f.csv.gz\n",
    "    mkdir /Data/$f\n",
    "    cp $f.csv.gz $f\n",
    "    cd /Data/$f\n",
    "    gzip -d $f.csv.gz\n",
    "    split -l 50000 $f.csv fp_\n",
    "    rm $f.csv\n",
    "    cd /Data\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting emr_getheaders.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile emr_getheaders.sh\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "rm *.csv.gz\n",
    "files=(https://www.dropbox.com/s/ikqguxu91yf8fiz/train_numeric.csv.gz\\\n",
    "        https://www.dropbox.com/s/9kl3m7ssx7uhnda/test_numeric.csv.gz\\\n",
    "        https://www.dropbox.com/s/59vl5rch97v7t6c/test_date.csv.gz\\\n",
    "        https://www.dropbox.com/s/anxa8zqei68uidp/train_date.csv.gz\\\n",
    "        https://www.dropbox.com/s/y6syumln123x4lb/train_categorical.csv.gz\\\n",
    "        https://www.dropbox.com/s/eczq1y1uzzy66el/test_categorical.csv.gz)\n",
    "for f in ${files[@]}\n",
    "do\n",
    "    wget $f\n",
    "done\n",
    "\n",
    "files=(train_numeric test_numeric train_date test_date train_categorical test_categorical)\n",
    "for f in ${files[@]}\n",
    "do\n",
    "    cat $f.csv.gz | gzip -d | head -n 1 > $f.csv\n",
    "    rm $f.csv.gz\n",
    "    gzip $f.csv\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting emr_getfiles.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile emr_getfiles.sh\n",
    "#!/usr/bin/env bash\n",
    "nn1=$1\n",
    "\n",
    "hadoop distcp s3a://brandonshurick/Data/train_categorical hdfs://$nn1/user/hadoop/train_categorical\n",
    "hadoop distcp s3a://brandonshurick/Data/train_numeric hdfs://$nn1/user/hadoop/train_numeric\n",
    "hadoop distcp s3a://brandonshurick/Data/train_date hdfs://$nn1/user/hadoop/train_date\n",
    "\n",
    "hadoop distcp s3a://brandonshurick/Data/test_categorical hdfs://$nn1/user/hadoop/test_categorical\n",
    "hadoop distcp s3a://brandonshurick/Data/test_numeric hdfs://$nn1/user/hadoop/test_numeric\n",
    "hadoop distcp s3a://brandonshurick/Data/test_date hdfs://$nn1/user/hadoop/test_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple, defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hashlib\n",
    "from math import log, exp, sqrt\n",
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# for viewing data samples\n",
    "pd.options.display.max_rows = 5000\n",
    "\n",
    "# base directory\n",
    "DIR = '/Data/testfile_'\n",
    "SUFFIX = '.csv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import SparseVector\n",
    "def createOneHotDict(inputData, header):\n",
    "    \"\"\"Creates a one-hot-encoder dictionary based on the input data.\n",
    "\n",
    "    Args:\n",
    "        inputData (RDD of lists of (str, str)): An RDD of observations where each observation is\n",
    "            made up of a list of (header, value) tuples.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are (header, value) tuples and map to values that are\n",
    "            unique integers.\n",
    "    \"\"\"\n",
    "    distinctFeatures = inputData.flatMap(lambda x: zip(header[1:], x[1])).distinct()\n",
    "    outputDict = distinctFeatures.zipWithIndex().collectAsMap()\n",
    "    return outputDict\n",
    "\n",
    "\n",
    "def createSparseVector(allFeatures, categoricalOHE, numNumericFeats):\n",
    "    \"\"\"Produce a sparse vector from all features \n",
    "\n",
    "    Args:\n",
    "        rawFeats (lists of (str, str), float): One list per feature type\n",
    "        categoricalOHE (OHE): namedtuple with OHEDict, header, and numFeats\n",
    "        numNumericFeats (int): count of numeric features\n",
    "\n",
    "    Returns:\n",
    "        SparseVector: An RDD with id and SparseVector with indicies equal \n",
    "            to the unique identifiers for the (featureID, value) combinations that occur in the \n",
    "            observation and with values equal to 1.0.\n",
    "    \"\"\"\n",
    "    categorical, numeric = allFeatures\n",
    "    totalFeatures = categoricalOHE.numFeats+numNumericFeats\n",
    "    ref = {}\n",
    "    \n",
    "    # add OHE data\n",
    "    for k in categorical:\n",
    "        if len(k[1])>0:\n",
    "            ref.update({categoricalOHE.OHEDict[k]:1})\n",
    "        \n",
    "    # add numeric data\n",
    "    for i,n in enumerate(numeric):\n",
    "        try:\n",
    "            nfloat = float(n)\n",
    "        except ValueError:\n",
    "            nfloat = 0.0\n",
    "        ref.update({(categoricalOHE.numFeats+i):nfloat})\n",
    "    \n",
    "    # return sparse vector\n",
    "    return SparseVector(totalFeatures, ref)\n",
    "\n",
    "\n",
    "def parseOHEPoint(point, categoricalOHE, numNumericFeats):\n",
    "    \"\"\"Obtain the label and feature vector for this raw observation.\n",
    "\n",
    "    Args:\n",
    "        point (lists of (str, str), float, int): joined point with each data type and outcome\n",
    "        categoricalOHE (OHE): namedtuple with OHEDict, header, and numFeats\n",
    "\n",
    "    Returns:\n",
    "        ID, LabeledPoint: Contains the label for the observation and the one-hot-encoding of the\n",
    "            raw features based on the provided OHE dictionary.\n",
    "    \"\"\"\n",
    "    Id = point[0]\n",
    "    (categorical, numeric), outcome = point[1]\n",
    "    categoricalMapped = [ (h,v) for h,v in zip(categoricalOHE.header[1:], categorical) ]\n",
    "    lp = LabeledPoint(outcome, createSparseVector((categoricalMapped, numeric), \n",
    "                                                  categoricalOHE, \n",
    "                                                  numNumericFeats))\n",
    "    return (Id, lp)\n",
    "\n",
    "\n",
    "def getP(x, w, intercept):\n",
    "    \"\"\"Calculate the probability for an observation given a set of weights and intercept.\n",
    "\n",
    "    Note:\n",
    "        We'll bound our raw prediction between 20 and -20 for numerical purposes.\n",
    "\n",
    "    Args:\n",
    "        x (SparseVector): A vector with values of 1.0 for features that exist in this\n",
    "            observation and 0.0 otherwise.\n",
    "        w (DenseVector): A vector of weights (betas) for the model.\n",
    "        intercept (float): The model's intercept.\n",
    "\n",
    "    Returns:\n",
    "        float: A probability between 0 and 1.\n",
    "    \"\"\"\n",
    "    rawPrediction = x.dot(w)+intercept\n",
    "\n",
    "    # Bound the raw prediction value\n",
    "    rawPrediction = min(rawPrediction, 20)\n",
    "    rawPrediction = max(rawPrediction, -20)\n",
    "    return 1.0/(1+exp(-rawPrediction))\n",
    "\n",
    "\n",
    "def computeLogLoss(p, y):\n",
    "    \"\"\"Calculates the value of log loss for a given probabilty and label.\n",
    "\n",
    "    Note:\n",
    "        log(0) is undefined, so when p is 0 we need to add a small value (epsilon) to it\n",
    "        and when p is 1 we need to subtract a small value (epsilon) from it.\n",
    "\n",
    "    Args:\n",
    "        p (float): A probabilty between 0 and 1.\n",
    "        y (int): A label.  Takes on the values 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "        float: The log loss value.\n",
    "    \"\"\"\n",
    "    epsilon = 10e-12\n",
    "    if p==0:\n",
    "        p+=epsilon\n",
    "    elif p==1:\n",
    "        p-=epsilon\n",
    "    if y==1:\n",
    "        return -log(p)\n",
    "    elif y==0:\n",
    "        return -log(1-p)\n",
    "    else:\n",
    "        raise Exception('y not in {0,1}')\n",
    "\n",
    "\n",
    "def getP(x, w, intercept):\n",
    "    \"\"\"Calculate the probability for an observation given a set of weights and intercept.\n",
    "\n",
    "    Note:\n",
    "        We'll bound our raw prediction between 20 and -20 for numerical purposes.\n",
    "\n",
    "    Args:\n",
    "        x (SparseVector): A vector with values of 1.0 for features that exist in this\n",
    "            observation and 0.0 otherwise.\n",
    "        w (DenseVector): A vector of weights (betas) for the model.\n",
    "        intercept (float): The model's intercept.\n",
    "\n",
    "    Returns:\n",
    "        float: A probability between 0 and 1.\n",
    "    \"\"\"\n",
    "    rawPrediction = x.dot(w)+intercept\n",
    "\n",
    "    # Bound the raw prediction value\n",
    "    rawPrediction = min(rawPrediction, 20)\n",
    "    rawPrediction = max(rawPrediction, -20)\n",
    "    return 1.0/(1+exp(-rawPrediction))\n",
    "\n",
    "\n",
    "def getMccData(p, x):\n",
    "    tp, tn, fp, fn = [0]*4\n",
    "    v = int((p>0.5)*1)\n",
    "    a = int(x)\n",
    "    if v==1 and v==a:\n",
    "        tp = 1\n",
    "    elif v==0 and v==a:\n",
    "        tn = 1\n",
    "    elif v==1 and v!=a:\n",
    "        fp = 1\n",
    "    elif v==0 and v!=a:\n",
    "        fn = 1\n",
    "    return tp, tn, fp, fn\n",
    "\n",
    "\n",
    "def getMcc(data_px):\n",
    "    mccdata = data_px.map(lambda x: getMccData(x[0],x[1]))\n",
    "    tp = mccdata.map(lambda x: x[0]).sum()\n",
    "    tn = mccdata.map(lambda x: x[1]).sum()\n",
    "    fp = mccdata.map(lambda x: x[2]).sum()\n",
    "    fn = mccdata.map(lambda x: x[3]).sum()\n",
    "    mcc_num = (tp*tn) - (fp*fn)\n",
    "    mcc_base = sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "    try:\n",
    "        return mcc_num*1.0/mcc_base\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def print_scores(model):\n",
    "    print 'Model log-loss: {}'.format(model.scores.logloss)\n",
    "    print 'Model accuracy: {}'.format(model.scores.accuracy)\n",
    "    print 'Model MCC: {}'.format(model.scores.mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# named tuples for code readability \n",
    "Files = namedtuple('filelist',['categorical','date','numeric'])\n",
    "Header = namedtuple('header',['categorical','date','numeric'])\n",
    "Data = namedtuple('data',['categorical','date','numeric','outcome'])\n",
    "Model = namedtuple('model',['model','scores'])\n",
    "Scores = namedtuple('scores',['logloss','accuracy','mcc'])\n",
    "OHE = namedtuple('ohe',['OHEDict','header','numFeats'])\n",
    "\n",
    "# declare file locations \n",
    "train_files = Files(DIR+'train_categorical'+SUFFIX,\n",
    "                DIR+'train_date'+SUFFIX,\n",
    "                DIR+'train_numeric'+SUFFIX)\n",
    "test_files = Files(DIR+'test_categorical'+SUFFIX,\n",
    "                DIR+'test_date'+SUFFIX,\n",
    "                DIR+'test_numeric'+SUFFIX)\n",
    "train_files_header = Files(DIR+'train_categorical.csv.gz',\n",
    "                DIR+'train_date.csv.gz',\n",
    "                DIR+'train_numeric.csv.gz')\n",
    "test_files_header = Files(DIR+'test_categorical.csv.gz',\n",
    "                DIR+'test_date.csv.gz',\n",
    "                DIR+'test_numeric.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cache raw data \n",
    "train_raw = Data(sc.textFile(train_files.categorical).cache(),\n",
    "                 sc.textFile(train_files.date).cache(),\n",
    "                 sc.textFile(train_files.numeric).cache(),None)\n",
    "test_raw = Data(sc.textFile(test_files.categorical).cache(),\n",
    "                sc.textFile(test_files.date).cache(),\n",
    "                sc.textFile(test_files.numeric).cache(),None)\n",
    "\n",
    "# headers \n",
    "get_header = lambda x: list(pd.read_csv(x,nrows=0).columns.values)\n",
    "train_headers = Header(get_header(train_files_header.categorical),\n",
    "                       get_header(train_files_header.date),\n",
    "                       get_header(train_files_header.numeric))\n",
    "test_headers = Header(get_header(test_files_header.categorical),\n",
    "                       get_header(test_files_header.date),\n",
    "                       get_header(test_files_header.numeric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = train_raw.categorical.first()\n",
    "_ = train_raw.numeric.first()\n",
    "_ = train_raw.date.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# filter out first row\n",
    "remove_header = lambda x: x.split(',')[0]!='Id'\n",
    "\n",
    "\n",
    "# return outcome from numeric data\n",
    "get_outcome = lambda (k,v): (k, int(v[-1]))\n",
    "subtract_outcome = lambda (k,v): (k, v[:-1])\n",
    "\n",
    "\n",
    "# split key and fields\n",
    "id_split = lambda x: (int(x.split(',')[0]),x.split(',')[1:])\n",
    "def header_key(x, headers):\n",
    "    ''' split id from fields\n",
    "        emit header as key\n",
    "    '''\n",
    "    id_int = x[0]\n",
    "    fields = x[1]\n",
    "    for i, f in enumerate(fields):\n",
    "        h = headers[1:][i]\n",
    "        yield (h, [(id_int, f)])\n",
    "\n",
    "\n",
    "# gather numeric features\n",
    "def convert_numeric(x):\n",
    "    ''' Gather numeric features '''\n",
    "    k,v = x\n",
    "    num = float(v[0][1]) if len(v[0][1])>0 else 0\n",
    "    return (k, [(v[0][0], num)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter header from data and get outcome\n",
    "train_filtered = Data(\n",
    "    train_raw.categorical.filter(remove_header).map(id_split),\n",
    "    train_raw.date.filter(remove_header).map(id_split),\n",
    "    train_raw.numeric.filter(remove_header).map(id_split),\n",
    "    train_raw.numeric.filter(remove_header).map(id_split).map(get_outcome)\n",
    ")\n",
    "test_filtered = Data(\n",
    "    test_raw.categorical.filter(remove_header).map(id_split),\n",
    "    test_raw.date.filter(remove_header).map(id_split),\n",
    "    test_raw.numeric.filter(remove_header).map(id_split),\n",
    "    None #unknown\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# group data by column \n",
    "train_explode = Data(\n",
    "    train_filtered.categorical.flatMap(lambda x: header_key(x, train_headers.categorical)),\n",
    "    train_filtered.date.flatMap(lambda x: header_key(x, train_headers.date)),\n",
    "    train_filtered.numeric.flatMap(lambda x: header_key(x, train_headers.numeric)).map(convert_numeric),\n",
    "    train_filtered.outcome\n",
    ")\n",
    "test_explode = Data(\n",
    "    test_filtered.categorical.flatMap(lambda x: header_key(x, train_headers.categorical)),\n",
    "    test_filtered.date.flatMap(lambda x: header_key(x, train_headers.date)),\n",
    "    test_filtered.numeric.flatMap(lambda x: header_key(x, train_headers.numeric)).map(convert_numeric),\n",
    "    None #unknown\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create key-val RDDs\n",
    "train_rdd = Data(\n",
    "    train_filtered.categorical,\n",
    "    train_filtered.date,\n",
    "    train_filtered.numeric.map(subtract_outcome),\n",
    "    train_filtered.outcome\n",
    ")\n",
    "test_rdd = Data(\n",
    "    test_filtered.categorical,\n",
    "    test_filtered.date,\n",
    "    test_filtered.numeric,\n",
    "    None #unknown\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# explore categorical columns\n",
    "def groupby_col(x):\n",
    "    k, v= x\n",
    "    colval = v[0][1]\n",
    "    return ((k,colval),1)\n",
    "col_counts = train_explode.categorical.filter(lambda (k,x): len(x[0][1])>0).\\\n",
    "                          map(groupby_col).reduceByKey(lambda x,y: x+y).\\\n",
    "                          collect()\n",
    "col_counts.sort(key=lambda x: x[0])\n",
    "total_count = train_explode.categorical.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('L0_S10_F215', u'T1'), 3),\n",
       " (('L0_S10_F217', u'T1'), 3),\n",
       " (('L0_S10_F218', u'T16777232'), 1),\n",
       " (('L0_S10_F218', u'T16777557'), 1),\n",
       " (('L0_S10_F218', u'T8'), 1),\n",
       " (('L0_S10_F220', u'T1'), 3),\n",
       " (('L0_S10_F222', u'T1'), 3),\n",
       " (('L0_S10_F223', u'T16777232'), 1),\n",
       " (('L0_S10_F223', u'T16777557'), 1),\n",
       " (('L0_S10_F223', u'T8'), 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_counts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21397860"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# explore numeric columns\n",
    "def groupby_col(x):\n",
    "    k, v= x\n",
    "    colval = v[0][1]\n",
    "    return (k, colval)\n",
    "col_sums = train_explode.numeric.map(groupby_col).reduceByKey(lambda x,y: x+y)\n",
    "col_totals = train_explode.numeric.map(lambda (k,v): (k,1)).reduceByKey(lambda x,y: x+y)\n",
    "col_totals = col_sums.join(col_totals).collect()\n",
    "col_means = [ (x, y[0]/y[1]) for x,y in col_totals ]\n",
    "col_means.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('L0_S0_F0', -0.001087908790879087),\n",
       " ('L0_S0_F10', 0.0023501350135013213),\n",
       " ('L0_S0_F12', 0.0001988198819882035),\n",
       " ('L0_S0_F14', 0.0013297329732973294),\n",
       " ('L0_S0_F16', -0.00045274527452745497),\n",
       " ('L0_S0_F18', 0.00015111511151114567),\n",
       " ('L0_S0_F2', -0.0014882488248824874),\n",
       " ('L0_S0_F20', 0.0027981798179817995),\n",
       " ('L0_S0_F22', 0.002806180618061808),\n",
       " ('L0_S0_F4', 0.0005809580958096)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_means[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Field</th>\n",
       "      <th>NumVal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L0_S0_F0</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L0_S0_F2</td>\n",
       "      <td>-0.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L0_S0_F4</td>\n",
       "      <td>-0.197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L0_S0_F6</td>\n",
       "      <td>-0.179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L0_S0_F8</td>\n",
       "      <td>0.118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>L0_S0_F10</td>\n",
       "      <td>0.116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>L0_S0_F12</td>\n",
       "      <td>-0.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>L0_S0_F14</td>\n",
       "      <td>-0.032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>L0_S0_F16</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>L0_S0_F18</td>\n",
       "      <td>0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>L0_S0_F20</td>\n",
       "      <td>-0.273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>L0_S0_F22</td>\n",
       "      <td>-0.273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>L0_S1_F24</td>\n",
       "      <td>-0.271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>L0_S1_F28</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>L0_S2_F32</td>\n",
       "      <td>-0.213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Field  NumVal\n",
       "0    L0_S0_F0    0.03\n",
       "1    L0_S0_F2  -0.034\n",
       "2    L0_S0_F4  -0.197\n",
       "3    L0_S0_F6  -0.179\n",
       "4    L0_S0_F8   0.118\n",
       "5   L0_S0_F10   0.116\n",
       "6   L0_S0_F12  -0.015\n",
       "7   L0_S0_F14  -0.032\n",
       "8   L0_S0_F16    0.02\n",
       "9   L0_S0_F18   0.083\n",
       "10  L0_S0_F20  -0.273\n",
       "11  L0_S0_F22  -0.273\n",
       "12  L0_S1_F24  -0.271\n",
       "13  L0_S1_F28   0.167\n",
       "14  L0_S2_F32  -0.213"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at first field\n",
    "pd.DataFrame(zip(train_headers.numeric[1:], \n",
    "                 train_rdd.numeric.first()[1]),\n",
    "             columns=['Field','NumVal']).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate baseline model\n",
    "sum_response = train_explode.numeric.filter(lambda x: x[0]=='Response').\\\n",
    "                                     map(lambda x: x[1][0][1]).\\\n",
    "                                     reduce(lambda x,y: x+y)\n",
    "count_response = train_explode.numeric.filter(lambda x: x[0]=='Response').count()\n",
    "baseline = sum_response*1.0/count_response\n",
    "logloss = train_filtered.outcome.map(lambda x: computeLogLoss(baseline, x[1])).sum() / train_filtered.outcome.count()\n",
    "accuracy = 1-baseline\n",
    "mcc = getMcc(train_filtered.outcome.map(lambda x: (baseline, x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline log-loss: 0.0330609616109\n",
      "Baseline accuracy: 0.994699469947\n",
      "Baseline MCC: 0\n"
     ]
    }
   ],
   "source": [
    "print 'Baseline log-loss: {}'.format(logloss)\n",
    "print 'Baseline accuracy: {}'.format(accuracy)\n",
    "print 'Baseline MCC: {}'.format(mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup full dataset\n",
    "train_joined = train_rdd.categorical.join(train_rdd.numeric).join(train_rdd.outcome)\n",
    "oheDict = createOneHotDict(train_rdd.categorical, train_headers.categorical)\n",
    "numFeats = max(oheDict.values())\n",
    "categoricalOHE = OHE(oheDict, train_headers.categorical, numFeats)\n",
    "numNumericFeats = len(train_rdd.numeric.first()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_lp = train_joined.map(lambda x: parseOHEPoint(x, categoricalOHE, numNumericFeats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "training, val = training_lp.randomSplit([0.7, 0.3])\n",
    "train_params = {\n",
    "    'iterations':300, \n",
    "    'initialWeights':[1]*len(training.map(lambda x: x[1]).first().features), \n",
    "    'regParam':0.001, \n",
    "    'regType':'l2', \n",
    "    'intercept':True, \n",
    "    'validateData':False, \n",
    "    'corrections':10, \n",
    "    'tolerance':1e-06, \n",
    "    'numClasses':2\n",
    "}\n",
    "m = LogisticRegressionWithLBFGS().train(training.map(lambda x: x[1]), **train_params)\n",
    "p = val.map(lambda x: x[1]).map(lambda x: (getP(x.features, m.weights, m.intercept), x.label))\n",
    "s = Scores(\n",
    "    p.map(lambda x: computeLogLoss(x[0], x[1])).sum() / p.count(),\n",
    "    p.map(lambda x: int(x[0]>0.5)==int(x[1])).sum()*1.0 / p.count(),\n",
    "    getMcc(p)\n",
    ")\n",
    "model1 = Model(m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_scores(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import GradientBoostedTrees\n",
    "data = train_rdd.numeric.join(train_rdd.outcome).map(convertPointNumeric)\n",
    "training, val = data.randomSplit([0.7, 0.3])\n",
    "gb_params = {\n",
    "    'categoricalFeaturesInfo':{},\n",
    "    'numIterations':300,\n",
    "    'loss':'leastSquaresError', #logLoss\n",
    "    'learningRate':0.1, \n",
    "    'maxDepth':3, \n",
    "    'maxBins':32,\n",
    "}\n",
    "m = GradientBoostedTrees.trainClassifier(training, **gb_params)\n",
    "p = m.predict(val.map(lambda x: x.features))\n",
    "lp = val.map(lambda lp: lp.label).zip(p).map(lambda x: (x[1],x[0]))\n",
    "s = Scores(\n",
    "    lp.map(lambda x: computeLogLoss(x[0], x[1])).sum() / lp.count(),\n",
    "    lp.map(lambda x: int(x[0]>0.5)==int(x[1])).sum()*1.0 / lp.count(),\n",
    "    getMcc(lp)\n",
    ")\n",
    "model2 = Model(m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_scores(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
