{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple, defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hashlib\n",
    "from math import log, exp\n",
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# for viewing data samples\n",
    "pd.options.display.max_rows = 5000\n",
    "\n",
    "# base directory\n",
    "DIR = '/Data/testfile_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hashFunction(numBuckets, rawFeats, printMapping=False):\n",
    "    \"\"\"Calculate a feature dictionary for an observation's features based on hashing.\n",
    "\n",
    "    Note:\n",
    "        Use printMapping=True for debug purposes and to better understand how the hashing works.\n",
    "\n",
    "    Args:\n",
    "        numBuckets (int): Number of buckets to use as features.\n",
    "        rawFeats (list of (int, str)): A list of features for an observation.  Represented as\n",
    "            (featureID, value) tuples.\n",
    "        printMapping (bool, optional): If true, the mappings of featureString to index will be\n",
    "            printed.\n",
    "\n",
    "    Returns:\n",
    "        dict of int to float:  The keys will be integers which represent the buckets that the\n",
    "            features have been hashed to.  The value for a given key will contain the count of the\n",
    "            (featureID, value) tuples that have hashed to that key.\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    for ind, category in rawFeats:\n",
    "        featureString = category + str(ind)\n",
    "        mapping[featureString] = int(int(hashlib.md5(featureString).hexdigest(), 16) % numBuckets)\n",
    "    if(printMapping): print mapping\n",
    "    sparseFeatures = defaultdict(float)\n",
    "    for bucket in mapping.values():\n",
    "        sparseFeatures[bucket] += 1.0\n",
    "    return dict(sparseFeatures)\n",
    "\n",
    "\n",
    "def parseHashPoint(point, numBuckets):\n",
    "    \"\"\"Create a LabeledPoint for this observation using hashing.\n",
    "\n",
    "    Args:\n",
    "        point (str): A comma separated string where the first value is the label and the rest are\n",
    "            features.\n",
    "        numBuckets: The number of buckets to hash to.\n",
    "\n",
    "    Returns:\n",
    "        LabeledPoint: A LabeledPoint with a label (0.0 or 1.0) and a SparseVector of hashed\n",
    "            features.\n",
    "    \"\"\"\n",
    "    splits = point.split(',')\n",
    "    fields = [ (i,v) for i,v in enumerate(splits[1:]) ]\n",
    "    vec = SparseVector(numBuckets, hashFunction(numBuckets, fields))\n",
    "    return LabeledPoint(splits[0], vec)\n",
    "\n",
    "\n",
    "def getP(x, w, intercept):\n",
    "    \"\"\"Calculate the probability for an observation given a set of weights and intercept.\n",
    "\n",
    "    Note:\n",
    "        We'll bound our raw prediction between 20 and -20 for numerical purposes.\n",
    "\n",
    "    Args:\n",
    "        x (SparseVector): A vector with values of 1.0 for features that exist in this\n",
    "            observation and 0.0 otherwise.\n",
    "        w (DenseVector): A vector of weights (betas) for the model.\n",
    "        intercept (float): The model's intercept.\n",
    "\n",
    "    Returns:\n",
    "        float: A probability between 0 and 1.\n",
    "    \"\"\"\n",
    "    rawPrediction = x.dot(w)+intercept\n",
    "\n",
    "    # Bound the raw prediction value\n",
    "    rawPrediction = min(rawPrediction, 20)\n",
    "    rawPrediction = max(rawPrediction, -20)\n",
    "    return 1.0/(1+exp(-rawPrediction))\n",
    "\n",
    "\n",
    "def computeLogLoss(p, y):\n",
    "    \"\"\"Calculates the value of log loss for a given probabilty and label.\n",
    "\n",
    "    Note:\n",
    "        log(0) is undefined, so when p is 0 we need to add a small value (epsilon) to it\n",
    "        and when p is 1 we need to subtract a small value (epsilon) from it.\n",
    "\n",
    "    Args:\n",
    "        p (float): A probabilty between 0 and 1.\n",
    "        y (int): A label.  Takes on the values 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "        float: The log loss value.\n",
    "    \"\"\"\n",
    "    epsilon = 10e-12\n",
    "    if p==0:\n",
    "        p+=epsilon\n",
    "    elif p==1:\n",
    "        p-=epsilon\n",
    "    if y==1:\n",
    "        return -log(p)\n",
    "    elif y==0:\n",
    "        return -log(1-p)\n",
    "    else:\n",
    "        raise Exception('y not in {0,1}')\n",
    "\n",
    "\n",
    "def getP(x, w, intercept):\n",
    "    \"\"\"Calculate the probability for an observation given a set of weights and intercept.\n",
    "\n",
    "    Note:\n",
    "        We'll bound our raw prediction between 20 and -20 for numerical purposes.\n",
    "\n",
    "    Args:\n",
    "        x (SparseVector): A vector with values of 1.0 for features that exist in this\n",
    "            observation and 0.0 otherwise.\n",
    "        w (DenseVector): A vector of weights (betas) for the model.\n",
    "        intercept (float): The model's intercept.\n",
    "\n",
    "    Returns:\n",
    "        float: A probability between 0 and 1.\n",
    "    \"\"\"\n",
    "    rawPrediction = x.dot(w)+intercept\n",
    "\n",
    "    # Bound the raw prediction value\n",
    "    rawPrediction = min(rawPrediction, 20)\n",
    "    rawPrediction = max(rawPrediction, -20)\n",
    "    return 1.0/(1+exp(-rawPrediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# named tuples for code readability \n",
    "Files = namedtuple('filelist',['categorical','date','numeric'])\n",
    "Header = namedtuple('header',['categorical','date','numeric'])\n",
    "Data = namedtuple('data',['categorical','date','numeric','outcome'])\n",
    "\n",
    "# declare train file locations \n",
    "train_files = Files(DIR+'train_categorical.csv.gz',\n",
    "                DIR+'train_date.csv.gz',\n",
    "                DIR+'train_numeric.csv.gz')\n",
    "test_files = Files(DIR+'test_categorical.csv.gz',\n",
    "                DIR+'test_date.csv.gz',\n",
    "                DIR+'test_numeric.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cache raw data \n",
    "train_raw = Data(sc.textFile(train_files.categorical).cache(),\n",
    "                 sc.textFile(train_files.date).cache(),\n",
    "                 sc.textFile(train_files.numeric).cache(),None)\n",
    "test_raw = Data(sc.textFile(test_files.categorical).cache(),\n",
    "                sc.textFile(test_files.date).cache(),\n",
    "                sc.textFile(test_files.numeric).cache(),None)\n",
    "\n",
    "# headers \n",
    "get_header = lambda x: list(pd.read_csv(x,nrows=0).columns.values)\n",
    "train_headers = Header(get_header(train_files.categorical),\n",
    "                       get_header(train_files.date),\n",
    "                       get_header(train_files.numeric))\n",
    "test_headers = Header(get_header(test_files.categorical),\n",
    "                       get_header(test_files.date),\n",
    "                       get_header(test_files.numeric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# filter out first row\n",
    "remove_header = lambda x: x.split(',')[0]!='Id'\n",
    "\n",
    "\n",
    "# return outcome from numeric data\n",
    "get_outcome = lambda (k,v): (k, int(v[-1]))\n",
    "subtract_outcome = lambda (k,v): (k, v[:-1])\n",
    "\n",
    "\n",
    "# split key and fields\n",
    "id_split = lambda x: (int(x.split(',')[0]),x.split(',')[1:])\n",
    "def header_key(x, headers):\n",
    "    ''' split id from fields\n",
    "        emit header as key\n",
    "    '''\n",
    "    id_int = x[0]\n",
    "    fields = x[1]\n",
    "    for i, f in enumerate(fields):\n",
    "        h = headers[1:][i]\n",
    "        yield (h, [(id_int, f)])\n",
    "\n",
    "\n",
    "# gather numeric features\n",
    "def convert_numeric(x):\n",
    "    ''' Gather numeric features '''\n",
    "    k,v = x\n",
    "    num = float(v[0][1]) if len(v[0][1])>0 else 0\n",
    "    return (k, [(v[0][0], num)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter header from data and get outcome\n",
    "train_filtered = Data(\n",
    "    train_raw.categorical.filter(remove_header).map(id_split),\n",
    "    train_raw.date.filter(remove_header).map(id_split),\n",
    "    train_raw.numeric.filter(remove_header).map(id_split),\n",
    "    train_raw.numeric.filter(remove_header).map(id_split).map(get_outcome)\n",
    ")\n",
    "test_filtered = Data(\n",
    "    test_raw.categorical.filter(remove_header).map(id_split),\n",
    "    test_raw.date.filter(remove_header).map(id_split),\n",
    "    test_raw.numeric.filter(remove_header).map(id_split),\n",
    "    None #unknown\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# group data by column \n",
    "train_explode = Data(\n",
    "    train_filtered.categorical.flatMap(lambda x: header_key(x, train_headers.categorical)).repartition(100),\n",
    "    train_filtered.date.flatMap(lambda x: header_key(x, train_headers.date)).repartition(100),\n",
    "    train_filtered.numeric.flatMap(lambda x: header_key(x, train_headers.numeric)).map(convert_numeric).repartition(100),\n",
    "    train_filtered.outcome\n",
    ")\n",
    "test_explode = Data(\n",
    "    test_filtered.categorical.flatMap(lambda x: header_key(x, train_headers.categorical)).repartition(100),\n",
    "    test_filtered.date.flatMap(lambda x: header_key(x, train_headers.date)).repartition(100),\n",
    "    test_filtered.numeric.flatMap(lambda x: header_key(x, train_headers.numeric)).map(convert_numeric).repartition(100),\n",
    "    None #unknown\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create key-val RDDs\n",
    "train_rdd = Data(\n",
    "    train_filtered.categorical.repartition(100),\n",
    "    train_filtered.date.repartition(100),\n",
    "    train_filtered.numeric.map(subtract_outcome).repartition(100),\n",
    "    train_filtered.outcome.repartition(100)\n",
    ")\n",
    "test_rdd = Data(\n",
    "    test_filtered.categorical.repartition(100),\n",
    "    test_filtered.date.repartition(100),\n",
    "    test_filtered.numeric.repartition(100),\n",
    "    None #unknown\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# explore categorical columns\n",
    "def groupby_col(x):\n",
    "    k, v= x\n",
    "    colval = v[0][1]\n",
    "    return ((k,colval),1)\n",
    "col_counts = train_explode.categorical.filter(lambda (k,x): len(x[0][1])>0).\\\n",
    "                          map(groupby_col).reduceByKey(lambda x,y: x+y).\\\n",
    "                          collect()\n",
    "col_counts.sort(key=lambda x: x[0][1])\n",
    "total_count = train_explode.categorical.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('L3_S32_F3854', u'T-2147481664'), 1),\n",
       " (('L3_S32_F3854', u'T-21474819'), 1),\n",
       " (('L3_S32_F3854', u'T-2147482432'), 17),\n",
       " (('L3_S32_F3854', u'T-2147482688'), 23),\n",
       " (('L3_S32_F3854', u'T-2147482816'), 33),\n",
       " (('L1_S24_F1524', u'T-2147483648'), 1),\n",
       " (('L3_S42_F4039', u'T-2147483648'), 1),\n",
       " (('L3_S42_F4031', u'T-2147483648'), 1),\n",
       " (('L3_S42_F4035', u'T-2147483648'), 1),\n",
       " (('L1_S24_F1543', u'T-2147483648'), 1)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_counts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "213997860"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# explore numeric columns\n",
    "def groupby_col(x):\n",
    "    k, v= x\n",
    "    colval = v[0][1]\n",
    "    return (k, colval)\n",
    "col_sums = train_explode.numeric.map(groupby_col).reduceByKey(lambda x,y: x+y).collect()\n",
    "col_totals = train_explode.numeric.map(lambda (k,v): (k,1)).reduceByKey(lambda x,y: x+y).collect()\n",
    "col_sums.sort(); col_totals.sort()\n",
    "col_means = [ (x[0], x[1]/y[1]) for x,y in zip(col_sums, col_totals) if x[0]==y[0]]\n",
    "col_means.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('L0_S0_F0', -0.0010608006080060795),\n",
       " ('L0_S0_F10', 0.0022263022630226296),\n",
       " ('L0_S0_F12', 0.000194271942719427),\n",
       " ('L0_S0_F14', 0.0012243322433224321),\n",
       " ('L0_S0_F16', -0.0008491784917849178),\n",
       " ('L0_S0_F18', -0.0005524055240552416),\n",
       " ('L0_S0_F2', -0.0014821548215482154),\n",
       " ('L0_S0_F20', 0.0024378643786437853),\n",
       " ('L0_S0_F22', 0.0024317243172431715),\n",
       " ('L0_S0_F4', -5.7000570005692755e-05)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_means[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Response', [(125, 0.0)])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_explode.numeric.filter(lambda x: x[0]=='Response').first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate baseline model\n",
    "sum_response = train_explode.numeric.filter(lambda x: x[0]=='Response').\\\n",
    "                                     map(lambda x: x[1][0][1]).\\\n",
    "                                     reduce(lambda x,y: x+y)\n",
    "count_response = train_explode.numeric.filter(lambda x: x[0]=='Response').count()\n",
    "baseline = sum_response*1.0/count_response\n",
    "logloss = train_filtered.outcome.map(lambda x: computeLogLoss(baseline, x[1])).sum() / train_filtered.outcome.count()\n",
    "accuracy = 1-baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline log-loss: 0.0350342985348\n",
      "Baseline accuracy: 0.994319943199\n"
     ]
    }
   ],
   "source": [
    "print 'Baseline log-loss: {}'.format(logloss)\n",
    "print 'Baseline accuracy: {}'.format(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "def convertPointNumeric(x):\n",
    "    k, v = x\n",
    "    v1, response = v\n",
    "    features = []\n",
    "    for v_i in v1:\n",
    "        if len(v_i)>0:\n",
    "            features.append(float(v_i))\n",
    "        else:\n",
    "            features.append(0)\n",
    "    return LabeledPoint(response, features)\n",
    "    \n",
    "data = train_rdd.numeric.join(train_rdd.outcome).map(convertPointNumeric)\n",
    "training, val = data.randomSplit([0.6, 0.4])\n",
    "train_params = {\n",
    "    'iterations':300, \n",
    "    'step':10.0, \n",
    "    'miniBatchFraction':1.0, \n",
    "    'initialWeights':None, \n",
    "    'regParam':0.001, \n",
    "    'regType':'l2', \n",
    "    'intercept':True, \n",
    "    'validateData':False, \n",
    "    'convergenceTol':0.001\n",
    "}\n",
    "model1 = LogisticRegressionWithSGD().train(training, **train_params)\n",
    "predictions = val.map(lambda x: (getP(x.features, model1.weights, model1.intercept), x.label))\n",
    "model1_logloss = predictions.map(lambda x: computeLogLoss(x[0], x[1])).sum() / predictions.count()\n",
    "model1_accuracy = predictions.map(lambda x: int(x[0]>0.5)==int(x[1])).sum()*1.0 / predictions.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model1 log-loss: 0.034410185753\n",
      "Model1 accuracy: 0.994434931507\n"
     ]
    }
   ],
   "source": [
    "print 'Model1 log-loss: {}'.format(model1_logloss)\n",
    "print 'Model1 accuracy: {}'.format(model1_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
