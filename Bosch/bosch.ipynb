{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple, defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hashlib\n",
    "from math import log, exp, sqrt\n",
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# for viewing data samples\n",
    "pd.options.display.max_rows = 5000\n",
    "\n",
    "# base directory\n",
    "DIR = '/Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hashFunction(numBuckets, rawFeats, printMapping=False):\n",
    "    \"\"\"Calculate a feature dictionary for an observation's features based on hashing.\n",
    "\n",
    "    Note:\n",
    "        Use printMapping=True for debug purposes and to better understand how the hashing works.\n",
    "\n",
    "    Args:\n",
    "        numBuckets (int): Number of buckets to use as features.\n",
    "        rawFeats (list of (int, str)): A list of features for an observation.  Represented as\n",
    "            (featureID, value) tuples.\n",
    "        printMapping (bool, optional): If true, the mappings of featureString to index will be\n",
    "            printed.\n",
    "\n",
    "    Returns:\n",
    "        dict of int to float:  The keys will be integers which represent the buckets that the\n",
    "            features have been hashed to.  The value for a given key will contain the count of the\n",
    "            (featureID, value) tuples that have hashed to that key.\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    for ind, category in rawFeats:\n",
    "        featureString = category + str(ind)\n",
    "        mapping[featureString] = int(int(hashlib.md5(featureString).hexdigest(), 16) % numBuckets)\n",
    "    if(printMapping): print mapping\n",
    "    sparseFeatures = defaultdict(float)\n",
    "    for bucket in mapping.values():\n",
    "        sparseFeatures[bucket] += 1.0\n",
    "    return dict(sparseFeatures)\n",
    "\n",
    "\n",
    "def parseHashPoint(point, numBuckets):\n",
    "    \"\"\"Create a LabeledPoint for this observation using hashing.\n",
    "\n",
    "    Args:\n",
    "        point (str): A comma separated string where the first value is the label and the rest are\n",
    "            features.\n",
    "        numBuckets: The number of buckets to hash to.\n",
    "\n",
    "    Returns:\n",
    "        LabeledPoint: A LabeledPoint with a label (0.0 or 1.0) and a SparseVector of hashed\n",
    "            features.\n",
    "    \"\"\"\n",
    "    splits = point.split(',')\n",
    "    fields = [ (i,v) for i,v in enumerate(splits[1:]) ]\n",
    "    vec = SparseVector(numBuckets, hashFunction(numBuckets, fields))\n",
    "    return LabeledPoint(splits[0], vec)\n",
    "\n",
    "\n",
    "def getP(x, w, intercept):\n",
    "    \"\"\"Calculate the probability for an observation given a set of weights and intercept.\n",
    "\n",
    "    Note:\n",
    "        We'll bound our raw prediction between 20 and -20 for numerical purposes.\n",
    "\n",
    "    Args:\n",
    "        x (SparseVector): A vector with values of 1.0 for features that exist in this\n",
    "            observation and 0.0 otherwise.\n",
    "        w (DenseVector): A vector of weights (betas) for the model.\n",
    "        intercept (float): The model's intercept.\n",
    "\n",
    "    Returns:\n",
    "        float: A probability between 0 and 1.\n",
    "    \"\"\"\n",
    "    rawPrediction = x.dot(w)+intercept\n",
    "\n",
    "    # Bound the raw prediction value\n",
    "    rawPrediction = min(rawPrediction, 20)\n",
    "    rawPrediction = max(rawPrediction, -20)\n",
    "    return 1.0/(1+exp(-rawPrediction))\n",
    "\n",
    "\n",
    "def computeLogLoss(p, y):\n",
    "    \"\"\"Calculates the value of log loss for a given probabilty and label.\n",
    "\n",
    "    Note:\n",
    "        log(0) is undefined, so when p is 0 we need to add a small value (epsilon) to it\n",
    "        and when p is 1 we need to subtract a small value (epsilon) from it.\n",
    "\n",
    "    Args:\n",
    "        p (float): A probabilty between 0 and 1.\n",
    "        y (int): A label.  Takes on the values 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "        float: The log loss value.\n",
    "    \"\"\"\n",
    "    epsilon = 10e-12\n",
    "    if p==0:\n",
    "        p+=epsilon\n",
    "    elif p==1:\n",
    "        p-=epsilon\n",
    "    if y==1:\n",
    "        return -log(p)\n",
    "    elif y==0:\n",
    "        return -log(1-p)\n",
    "    else:\n",
    "        raise Exception('y not in {0,1}')\n",
    "\n",
    "\n",
    "def getP(x, w, intercept):\n",
    "    \"\"\"Calculate the probability for an observation given a set of weights and intercept.\n",
    "\n",
    "    Note:\n",
    "        We'll bound our raw prediction between 20 and -20 for numerical purposes.\n",
    "\n",
    "    Args:\n",
    "        x (SparseVector): A vector with values of 1.0 for features that exist in this\n",
    "            observation and 0.0 otherwise.\n",
    "        w (DenseVector): A vector of weights (betas) for the model.\n",
    "        intercept (float): The model's intercept.\n",
    "\n",
    "    Returns:\n",
    "        float: A probability between 0 and 1.\n",
    "    \"\"\"\n",
    "    rawPrediction = x.dot(w)+intercept\n",
    "\n",
    "    # Bound the raw prediction value\n",
    "    rawPrediction = min(rawPrediction, 20)\n",
    "    rawPrediction = max(rawPrediction, -20)\n",
    "    return 1.0/(1+exp(-rawPrediction))\n",
    "\n",
    "\n",
    "def getMccData(p, x):\n",
    "    tp, tn, fp, fn = [0]*4\n",
    "    v = int((p>0.5)*1)\n",
    "    a = int(x)\n",
    "    if v==1 and v==a:\n",
    "        tp = 1\n",
    "    elif v==0 and v==a:\n",
    "        tn = 1\n",
    "    elif v==1 and v!=a:\n",
    "        fp = 1\n",
    "    elif v==0 and v!=a:\n",
    "        fn = 1\n",
    "    return tp, tn, fp, fn\n",
    "\n",
    "\n",
    "def getMcc(data_px):\n",
    "    mccdata = data_px.map(lambda x: getMccData(x[0],x[1]))\n",
    "    tp = mccdata.map(lambda x: x[0]).sum()\n",
    "    tn = mccdata.map(lambda x: x[1]).sum()\n",
    "    fp = mccdata.map(lambda x: x[2]).sum()\n",
    "    fn = mccdata.map(lambda x: x[3]).sum()\n",
    "    mcc_num = (tp*tn) - (fp*fn)\n",
    "    mcc_base = sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "    try:\n",
    "        return mcc_num*1.0/mcc_base\n",
    "    except ZeroDivisionError:\n",
    "        return \n",
    "\n",
    "\n",
    "def print_scores(model):\n",
    "    print 'Model1 log-loss: {}'.format(model.scores.logloss)\n",
    "    print 'Model1 accuracy: {}'.format(model.scores.accuracy)\n",
    "    print 'Model1 MCC: {}'.format(model.scores.mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# named tuples for code readability \n",
    "Files = namedtuple('filelist',['categorical','date','numeric'])\n",
    "Header = namedtuple('header',['categorical','date','numeric'])\n",
    "Data = namedtuple('data',['categorical','date','numeric','outcome'])\n",
    "Model = namedtuple('model',['model','scores'])\n",
    "Scores = namedtuple('scores',['logloss','accuracy','mcc'])\n",
    "\n",
    "# declare file locations \n",
    "train_files = Files(DIR+'train_categorical',\n",
    "                DIR+'train_date',\n",
    "                DIR+'train_numeric')\n",
    "test_files = Files(DIR+'test_categorical',\n",
    "                DIR+'test_date',\n",
    "                DIR+'test_numeric')\n",
    "train_files_header = Files(DIR+'train_categorical.csv.gz',\n",
    "                DIR+'train_date.csv.gz',\n",
    "                DIR+'train_numeric.csv.gz')\n",
    "test_files_header = Files(DIR+'test_categorical.csv.gz',\n",
    "                DIR+'test_date.csv.gz',\n",
    "                DIR+'test_numeric.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cache raw data \n",
    "train_raw = Data(sc.textFile(train_files.categorical).cache(),\n",
    "                 sc.textFile(train_files.date).cache(),\n",
    "                 sc.textFile(train_files.numeric).cache(),None)\n",
    "test_raw = Data(sc.textFile(test_files.categorical).cache(),\n",
    "                sc.textFile(test_files.date).cache(),\n",
    "                sc.textFile(test_files.numeric).cache(),None)\n",
    "\n",
    "# headers \n",
    "get_header = lambda x: list(pd.read_csv(x,nrows=0).columns.values)\n",
    "train_headers = Header(get_header(train_files_header.categorical),\n",
    "                       get_header(train_files_header.date),\n",
    "                       get_header(train_files_header.numeric))\n",
    "test_headers = Header(get_header(test_files_header.categorical),\n",
    "                       get_header(test_files_header.date),\n",
    "                       get_header(test_files_header.numeric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# filter out first row\n",
    "remove_header = lambda x: x.split(',')[0]!='Id'\n",
    "\n",
    "\n",
    "# return outcome from numeric data\n",
    "get_outcome = lambda (k,v): (k, int(v[-1]))\n",
    "subtract_outcome = lambda (k,v): (k, v[:-1])\n",
    "\n",
    "\n",
    "# split key and fields\n",
    "id_split = lambda x: (int(x.split(',')[0]),x.split(',')[1:])\n",
    "def header_key(x, headers):\n",
    "    ''' split id from fields\n",
    "        emit header as key\n",
    "    '''\n",
    "    id_int = x[0]\n",
    "    fields = x[1]\n",
    "    for i, f in enumerate(fields):\n",
    "        h = headers[1:][i]\n",
    "        yield (h, [(id_int, f)])\n",
    "\n",
    "\n",
    "# gather numeric features\n",
    "def convert_numeric(x):\n",
    "    ''' Gather numeric features '''\n",
    "    k,v = x\n",
    "    num = float(v[0][1]) if len(v[0][1])>0 else 0\n",
    "    return (k, [(v[0][0], num)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter header from data and get outcome\n",
    "train_filtered = Data(\n",
    "    train_raw.categorical.filter(remove_header).map(id_split),\n",
    "    train_raw.date.filter(remove_header).map(id_split),\n",
    "    train_raw.numeric.filter(remove_header).map(id_split),\n",
    "    train_raw.numeric.filter(remove_header).map(id_split).map(get_outcome)\n",
    ")\n",
    "test_filtered = Data(\n",
    "    test_raw.categorical.filter(remove_header).map(id_split),\n",
    "    test_raw.date.filter(remove_header).map(id_split),\n",
    "    test_raw.numeric.filter(remove_header).map(id_split),\n",
    "    None #unknown\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# group data by column \n",
    "train_explode = Data(\n",
    "    train_filtered.categorical.flatMap(lambda x: header_key(x, train_headers.categorical)),\n",
    "    train_filtered.date.flatMap(lambda x: header_key(x, train_headers.date)),\n",
    "    train_filtered.numeric.flatMap(lambda x: header_key(x, train_headers.numeric)).map(convert_numeric),\n",
    "    train_filtered.outcome\n",
    ")\n",
    "test_explode = Data(\n",
    "    test_filtered.categorical.flatMap(lambda x: header_key(x, train_headers.categorical)),\n",
    "    test_filtered.date.flatMap(lambda x: header_key(x, train_headers.date)),\n",
    "    test_filtered.numeric.flatMap(lambda x: header_key(x, train_headers.numeric)).map(convert_numeric),\n",
    "    None #unknown\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create key-val RDDs\n",
    "train_rdd = Data(\n",
    "    train_filtered.categorical,\n",
    "    train_filtered.date,\n",
    "    train_filtered.numeric.map(subtract_outcome),\n",
    "    train_filtered.outcome\n",
    ")\n",
    "test_rdd = Data(\n",
    "    test_filtered.categorical,\n",
    "    test_filtered.date,\n",
    "    test_filtered.numeric,\n",
    "    None #unknown\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# explore categorical columns\n",
    "def groupby_col(x):\n",
    "    k, v= x\n",
    "    colval = v[0][1]\n",
    "    return ((k,colval),1)\n",
    "col_counts = train_explode.categorical.filter(lambda (k,x): len(x[0][1])>0).\\\n",
    "                          map(groupby_col).reduceByKey(lambda x,y: x+y).\\\n",
    "                          collect()\n",
    "col_counts.sort(key=lambda x: x[0][1])\n",
    "total_count = train_explode.categorical.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('L0_S22_F545', u'T-18748192'), 2),\n",
       " (('L0_S9_F154', u'T-18748192'), 1),\n",
       " (('L3_S32_F3854', u'T-2147481664'), 34),\n",
       " (('L3_S32_F3854', u'T-21474819'), 5),\n",
       " (('L3_S32_F3854', u'T-2147482176'), 1),\n",
       " (('L3_S32_F3854', u'T-2147482432'), 375),\n",
       " (('L3_S32_F3854', u'T-21474825'), 12),\n",
       " (('L3_S32_F3854', u'T-2147482688'), 364),\n",
       " (('L3_S32_F3854', u'T-2147482816'), 385),\n",
       " (('L3_S32_F3854', u'T-2147482944'), 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_counts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2533218580"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# explore numeric columns\n",
    "def groupby_col(x):\n",
    "    k, v= x\n",
    "    colval = v[0][1]\n",
    "    return (k, colval)\n",
    "col_sums = train_explode.numeric.map(groupby_col).reduceByKey(lambda x,y: x+y).collect()\n",
    "col_totals = train_explode.numeric.map(lambda (k,v): (k,1)).reduceByKey(lambda x,y: x+y).collect()\n",
    "col_sums.sort(); col_totals.sort()\n",
    "col_means = [ (x[0], x[1]/y[1]) for x,y in zip(col_sums, col_totals) if x[0]==y[0]]\n",
    "col_means.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('L0_S0_F0', 3.10590016278761e-05),\n",
       " ('L0_S0_F10', 9.183803633712988e-05),\n",
       " ('L0_S0_F12', -2.5114319191515098e-05),\n",
       " ('L0_S0_F14', -0.00020445585078567285),\n",
       " ('L0_S0_F16', 3.826831240118771e-07),\n",
       " ('L0_S0_F18', 2.980366581710526e-06),\n",
       " ('L0_S0_F2', 5.196549600548068e-05),\n",
       " ('L0_S0_F20', 0.00016267749780991863),\n",
       " ('L0_S0_F22', 0.00016275775144519795),\n",
       " ('L0_S0_F4', 2.2800480170086108e-05)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_means[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Response', [(1799951, 0.0)])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_explode.numeric.filter(lambda x: x[0]=='Response').first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate baseline model\n",
    "sum_response = train_explode.numeric.filter(lambda x: x[0]=='Response').\\\n",
    "                                     map(lambda x: x[1][0][1]).\\\n",
    "                                     reduce(lambda x,y: x+y)\n",
    "count_response = train_explode.numeric.filter(lambda x: x[0]=='Response').count()\n",
    "baseline = sum_response*1.0/count_response\n",
    "logloss = train_filtered.outcome.map(lambda x: computeLogLoss(baseline, x[1])).sum() / train_filtered.outcome.count()\n",
    "accuracy = 1-baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline log-loss: 0.0357101958935\n",
      "Baseline accuracy: 0.994188792031\n"
     ]
    }
   ],
   "source": [
    "print 'Baseline log-loss: {}'.format(logloss)\n",
    "print 'Baseline accuracy: {}'.format(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "def convertPointNumeric(x):\n",
    "    k, v = x\n",
    "    v1, response = v\n",
    "    features = []\n",
    "    for v_i in v1:\n",
    "        if len(v_i)>0:\n",
    "            features.append(float(v_i))\n",
    "        else:\n",
    "            features.append(0)\n",
    "    return LabeledPoint(response, features)\n",
    "    \n",
    "data = train_rdd.numeric.join(train_rdd.outcome).map(convertPointNumeric)\n",
    "training, val = data.randomSplit([0.6, 0.4])\n",
    "train_params = {\n",
    "    'iterations':300, \n",
    "    'initialWeights':None, \n",
    "    'regParam':0.001, \n",
    "    'regType':'l2', \n",
    "    'intercept':True, \n",
    "    'validateData':False, \n",
    "    'corrections':10, \n",
    "    'tolerance':1e-06, \n",
    "    'numClasses':2\n",
    "}\n",
    "m = LogisticRegressionWithLBFGS().train(training, **train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = val.map(lambda x: (getP(x.features, m.weights, m.intercept), x.label))\n",
    "s = Scores(\n",
    "    p.map(lambda x: computeLogLoss(x[0], x[1])).sum() / p.count(),\n",
    "    p.map(lambda x: int(x[0]>0.5)==int(x[1])).sum()*1.0 / p.count(),\n",
    "    getMcc(p)\n",
    ")\n",
    "model1 = Model(m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model1 log-loss: 0.0343342090373\n",
      "Model1 accuracy: 0.9942148708\n",
      "Model1 MCC: 0.101547791483\n"
     ]
    }
   ],
   "source": [
    "print_scores(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import GradientBoostedTrees\n",
    "data = train_rdd.numeric.join(train_rdd.outcome).map(convertPointNumeric)\n",
    "training, val = data.randomSplit([0.6, 0.4])\n",
    "gb_params = {\n",
    "    'categoricalFeaturesInfo':{},\n",
    "    'numIterations':300,\n",
    "    'loss':'logLoss', \n",
    "    'learningRate':0.01, \n",
    "    'maxDepth':3, \n",
    "    'maxBins':32,\n",
    "}\n",
    "m = GradientBoostedTrees.trainClassifier(training, **gb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = m.predict(val.map(lambda x: x.features))\n",
    "lp = val.map(lambda lp: lp.label).zip(predictions)\n",
    "s = Scores(\n",
    "    lp.map(lambda x: computeLogLoss(x[0], x[1])).sum() / lp.count(),\n",
    "    lp.map(lambda x: int(x[0]>0.5)==int(x[1])).sum()*1.0 / lp.count(),\n",
    "    getMcc(lp)\n",
    ")\n",
    "model2 = Model(m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
