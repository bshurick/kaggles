{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# future functions\n",
    "from __future__ import print_function \n",
    "\n",
    "# core scipy and numpy\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# pandas \n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_columns = 135\n",
    "\n",
    "# encoder\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# manifold for embedding analysis\n",
    "from sklearn import manifold\n",
    "\n",
    "# Cross validation \n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Linear models \n",
    "from sklearn import linear_model\n",
    "\n",
    "# Forests\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "\n",
    "# SVR\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# vowpal wabbit\n",
    "from vowpalwabbit.sklearn_vw import VWRegressor\n",
    "\n",
    "# combinations with categorical features\n",
    "from itertools import combinations\n",
    "\n",
    "# matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# python helpers \n",
    "from collections import namedtuple\n",
    "from copy import copy\n",
    "\n",
    "# neural network\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Merge, Reshape\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "# XGboost for gradient-boosted decision trees\n",
    "import xgboost as xgb\n",
    "\n",
    "# logging\n",
    "import logging\n",
    "logging.basicConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loadfile = lambda x: pd.read_csv(x, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Files = namedtuple('Files',['train','test'])\n",
    "RawData = namedtuple('RawData',['train','test'])\n",
    "ProcessedData = namedtuple('ProcessedData',['train','test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load raw data\n",
    "rawfiles = Files(train='Data/train.csv.gz',\n",
    "            test='Data/test.csv.gz')\n",
    "raw = RawData(train=loadfile(rawfiles.train),\n",
    "              test=loadfile(rawfiles.test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-01397cd24f85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# concatenate all data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'SalePrice'\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m full = pd.concat((raw.train.loc[:, cols],\n\u001b[1;32m      4\u001b[0m                      raw.test))\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'processed' is not defined"
     ]
    }
   ],
   "source": [
    "# concatenate all data \n",
    "cols = [ c for c in raw.train.columns if c != 'SalePrice' ]\n",
    "full = pd.concat((raw.train.loc[:, cols],\n",
    "                     raw.test))\n",
    "\n",
    "# impute missing values\n",
    "fillfunc = lambda x: x.fillna(x.value_counts().iloc[0])\n",
    "full = full.apply(fillfunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create processed dataset\n",
    "processed = ProcessedData(train=raw.train.apply(fillfunc),\n",
    "                         test=raw.test.apply(fillfunc))\n",
    "processed.train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "outliers = [  30,   88,  197,  462,  495,  523,  557,  632,  691,  825,  874,\n",
    "             898,  968,  970, 1169, 1170, 1182, 1423, 1432, 1453 ] # from outlier analysis\n",
    "outlier_ids = raw.train.index.isin(outliers)\n",
    "processed = ProcessedData(train=processed.train.loc[~outlier_ids ,:],\n",
    "                            test=processed.test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create lookup for numeric categorical columns\n",
    "numeric_categorical = [\n",
    "    'MSSubClass',\n",
    "    'YearBuilt',\n",
    "    'YearRemodAdd',\n",
    "    'MoSold',\n",
    "    'YrSold',\n",
    "    'GarageYrBlt',\n",
    "    'OverallQual',\n",
    "    'OverallCond',\n",
    "    'MiscVal',\n",
    "]\n",
    "categorical_cols = list(processed.train.dtypes[processed.train.dtypes == \"object\"].index)\n",
    "categorical_cols += numeric_categorical\n",
    "continuous_cols = list(processed.train.dtypes[processed.train.dtypes != \"object\"].index)\n",
    "continuous_cols = [ c for c in continuous_cols if c not in numeric_categorical and c!='SalePrice' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for c1,c2 in combinations(categorical_cols, 2):\n",
    "    newc = c1+'_'+c2\n",
    "    processed.train.loc[:, newc] = processed.train[c1].astype(str) + '_' + processed.train[c2].astype(str)\n",
    "    processed.test.loc[:, newc] = processed.test[c1].astype(str) + '_' + processed.test[c2].astype(str)\n",
    "    categorical_cols.append(newc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# store important objects\n",
    "label_encoders = {}\n",
    "categorical_col_nums = []\n",
    "\n",
    "\n",
    "# encode data and store objects\n",
    "i = 0\n",
    "\n",
    "for col, col_data in full.iteritems():\n",
    "    \n",
    "    # handle categorical columns \n",
    "    if col_data.dtype == object or col in numeric_categorical:\n",
    "        le = LabelEncoder()\n",
    "        col_data = le.fit_transform(col_data)\n",
    "        categorical_col_nums.append(full.columns.get_loc(col))\n",
    "        label_encoders[col] = le\n",
    "    \n",
    "    # store objects\n",
    "    full[col] = col_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log transfoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_skew = lambda x: sp.stats.skewtest(x.dropna())[1]<0.05\n",
    "for c in continuous_cols:\n",
    "    if get_skew(processed.train[c]):\n",
    "        processed.train.loc[:, c] = np.log1p(processed.train[c])\n",
    "        processed.test.loc[:, c] = np.log1p(processed.test[c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup standard scaler and one hot encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# standard scaler\n",
    "continous_scaler = StandardScaler()\n",
    "c = continous_scaler.fit_transform(full.loc[:, continuous_cols].as_matrix())\n",
    "\n",
    "# minmax \n",
    "mm_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "m = mm_scaler.fit_transform(c)\n",
    "\n",
    "# transform data in full dataset\n",
    "full.loc[:, continuous_cols] = m \n",
    "\n",
    "# create one hot encoder\n",
    "ohe = OneHotEncoder(categorical_features=categorical_col_nums)\n",
    "o = ohe.fit_transform(full.as_matrix())\n",
    "\n",
    "# pca for continuous columns\n",
    "pca_cont_n = 1\n",
    "pca_cont = PCA(n_components=pca_cont_n)\n",
    "_ = pca_cont.fit(m)\n",
    "\n",
    "# pca for ohe columns\n",
    "pca_ohe_n = 1\n",
    "pca_ohe = PCA(n_components=pca_ohe_n)\n",
    "_ = pca_ohe.fit(o.A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical field analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categorical_field_analysis = []\n",
    "for c in categorical_cols:\n",
    "    # get pct freq coverage of top value \n",
    "    v = full[c].fillna('Unk').value_counts()\n",
    "    first_pct = v.iloc[0]*1.0/sum(v)\n",
    "\n",
    "    # get count of distinct values\n",
    "    distinct_vals = set(full[c].values)\n",
    "    d_cnt = len(distinct_vals)\n",
    "\n",
    "    # calculate logical freq  \n",
    "    logical_pct = 1.0/d_cnt\n",
    "        \n",
    "    # append\n",
    "    categorical_field_analysis.append((c, first_pct, logical_pct, d_cnt))\n",
    "\n",
    "categorical_field_analysis = pd.DataFrame(categorical_field_analysis,\n",
    "                                 columns=['Cat_Col',\n",
    "                                          'First_Freq',\n",
    "                                          'Logical_Freq',\n",
    "                                          'Distinct_Val_Cnt',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@np.vectorize\n",
    "def choose_m(freq, n):\n",
    "    for i in range(1,n+1):\n",
    "        if freq>=(1-float(i)/n):\n",
    "            x = i\n",
    "            break\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categorical_field_analysis['m'] =\\\n",
    "choose_m(categorical_field_analysis.First_Freq,\\\n",
    "categorical_field_analysis.Distinct_Val_Cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(categorical_field_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN model with entity embedding\n",
    "\n",
    "Choose reshape size for each categorical column. Push continuous columns through, as-is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_emb_nn_model(seed=2):\n",
    "    np.random.seed(seed)\n",
    "    models = []\n",
    "\n",
    "    for i, vals in categorical_field_analysis.T.iteritems():\n",
    "\n",
    "        # gather reshaping components\n",
    "        m = vals.Distinct_Val_Cnt\n",
    "        new_m = vals.m\n",
    "\n",
    "        # special cases \n",
    "        if vals.Cat_Col == 'YearBuilt':\n",
    "            new_m = 20\n",
    "        elif vals.Cat_Col == 'GarageYrBlt':\n",
    "            new_m = 10\n",
    "        elif vals.Cat_Col == 'YearRemodAdd':\n",
    "            new_m = 10\n",
    "        elif vals.Cat_Col == 'Neighborhood':\n",
    "            new_m = 15\n",
    "        elif vals.Cat_Col == 'HouseStyle':\n",
    "            new_m = 6\n",
    "\n",
    "        # create embedding for each feature\n",
    "        entity_model = Sequential()\n",
    "        entity_model.add(Embedding(m, new_m, input_length=1))\n",
    "        entity_model.add(Reshape(target_shape=(new_m,)))\n",
    "        models.append(entity_model)\n",
    "\n",
    "    n,m = full[continuous_cols].shape\n",
    "    m += pca_cont_n\n",
    "    continuous_model = Sequential()\n",
    "    continuous_model.add(Dense(m, input_dim=m))\n",
    "    models.append(continuous_model)\n",
    "\n",
    "    emb_model = Sequential()\n",
    "    emb_model.add(Merge(models, mode='concat'))\n",
    "    emb_model.add(Dropout(0.1))\n",
    "    emb_model.add(Dense(64, init='uniform'))\n",
    "    emb_model.add(Activation('relu'))\n",
    "    emb_model.add(Dropout(0.3))\n",
    "    emb_model.add(Dense(64, init='uniform'))\n",
    "    emb_model.add(Activation('relu'))\n",
    "    emb_model.add(Dropout(0.3))\n",
    "    emb_model.add(Dense(32, init='uniform'))\n",
    "    emb_model.add(Activation('relu'))\n",
    "    emb_model.add(Dropout(0.1))\n",
    "    emb_model.add(Dense(1))\n",
    "    emb_model.add(Activation('sigmoid'))\n",
    "    emb_model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return copy(emb_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanilla NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vanilla NN \n",
    "def gen_vanilla_nn(seed=2):\n",
    "    np.random.seed(seed)\n",
    "    n, m = preprocessing_X(X_train).shape\n",
    "    vanilla_nn_model = Sequential()\n",
    "    vanilla_nn_model.add(Dense(1500, init='uniform', input_shape=(m,)))\n",
    "    vanilla_nn_model.add(Activation('tanh'))\n",
    "    vanilla_nn_model.add(Dropout(0.3))\n",
    "    vanilla_nn_model.add(Dense(1500, init='uniform'))\n",
    "    vanilla_nn_model.add(Activation('tanh'))\n",
    "    vanilla_nn_model.add(Dropout(0.3))\n",
    "    vanilla_nn_model.add(Dense(1))\n",
    "    vanilla_nn_model.add(Activation('sigmoid'))\n",
    "    vanilla_nn_model.add(Dropout(0.01))\n",
    "    vanilla_nn_model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return copy(vanilla_nn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_y = np.log1p(processed.train.SalePrice)\n",
    "max_y = np.max(_y)\n",
    "\n",
    "\n",
    "def nn_preprocessing_X(X_dat):\n",
    "    X_out = []\n",
    "    \n",
    "    # categorical columns\n",
    "    for c in categorical_cols:\n",
    "        d = X_dat.loc[:, c].as_matrix()\n",
    "        dt = label_encoders[c].transform(d)\n",
    "        X_out.append(dt)\n",
    "    \n",
    "    # continuous columns\n",
    "    continuous_ss = continous_scaler.transform(X_dat.loc[:, continuous_cols].as_matrix())\n",
    "    continuous_mm = mm_scaler.transform(continuous_ss)\n",
    "    \n",
    "    # pca continuous \n",
    "    pca_Xdat_cont = pca_cont.transform(continuous_mm)\n",
    "    \n",
    "    X_out.append(np.concatenate((continuous_mm, pca_Xdat_cont), axis=1))\n",
    "    return X_out\n",
    "\n",
    "\n",
    "def preprocessing_X(X_dat):\n",
    "    X_dat = X_dat.copy()\n",
    "    \n",
    "    # categorical columns\n",
    "    for c in categorical_cols:\n",
    "        d = X_dat.loc[:, c].as_matrix()\n",
    "        dt = label_encoders[c].transform(d)\n",
    "        X_dat.loc[:, c] = dt\n",
    "    \n",
    "    # continuous columns\n",
    "    continuous_ss = continous_scaler.transform(X_dat.loc[:, continuous_cols].as_matrix())\n",
    "    continuous_mm = mm_scaler.transform(continuous_ss)\n",
    "    X_dat.loc[:, continuous_cols] = continuous_mm\n",
    "    \n",
    "    # pca continuous \n",
    "    pca_Xdat_cont = pca_cont.transform(continuous_mm)\n",
    "                             \n",
    "    # one hot encode \n",
    "    X_dat = ohe.transform(X_dat.as_matrix()).A\n",
    "    \n",
    "    # pca ohe \n",
    "    pca_Xdat_ohe = pca_ohe.transform(X_dat)\n",
    "    \n",
    "    return np.concatenate((X_dat, pca_Xdat_cont, pca_Xdat_ohe), axis=1)\n",
    "\n",
    "\n",
    "def preprocessing_Y(y_dat):\n",
    "    return np.log(y_dat.values+1)/max_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "processed.train.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ntrain = processed.train.shape[0]\n",
    "ntest = processed.test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrapper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SklearnWrapper(object):\n",
    "    def __init__(self, clf, seed=2, params={}):\n",
    "        try:\n",
    "            params['random_state'] = seed\n",
    "            self.clf = clf(**params)\n",
    "        except:\n",
    "            del params['random_state'] \n",
    "            self.clf = clf(**params)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        xtr = preprocessing_X(x_train)\n",
    "        ytr = preprocessing_Y(y_train).ravel()\n",
    "        self.clf.fit(xtr, ytr)\n",
    "\n",
    "    def predict(self, x):\n",
    "        xte = preprocessing_X(x)\n",
    "        return self.clf.predict(xte)\n",
    "\n",
    "\n",
    "class XgbWrapper:\n",
    "    def __init__(self, seed=2, params=None):\n",
    "        self.param = params\n",
    "        self.param['seed'] = seed\n",
    "        self.nrounds = params.pop('nrounds', 250)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        xtr = preprocessing_X(x_train)\n",
    "        ytr = preprocessing_Y(y_train).ravel()\n",
    "        dtrain = xgb.DMatrix(xtr, label=ytr)\n",
    "        self.gbdt = xgb.train(self.param, dtrain, self.nrounds)\n",
    "\n",
    "    def predict(self, x):\n",
    "        xte = preprocessing_X(x)\n",
    "        return self.gbdt.predict(xgb.DMatrix(xte))\n",
    "\n",
    "\n",
    "class NnWrapper:\n",
    "    def __init__(self, model, emb=True, nb_epoch=16, batch_size=8):\n",
    "        self.model = copy(model)\n",
    "        self.nb_epoch = nb_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.emb = emb\n",
    "    \n",
    "    def train(self, x_train, y_train):\n",
    "        if self.emb:\n",
    "            xtr = nn_preprocessing_X(x_train)\n",
    "        else:\n",
    "            xtr = preprocessing_X(x_train)\n",
    "        ytr = preprocessing_Y(y_train).ravel()\n",
    "        self.model.fit(xtr, ytr, \n",
    "                 nb_epoch = self.nb_epoch,\n",
    "                 batch_size = self.batch_size,\n",
    "                 verbose = 0)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        if self.emb:\n",
    "            xte = nn_preprocessing_X(x)\n",
    "        else:\n",
    "            xte = preprocessing_X(x)\n",
    "        return self.model.predict(xte).ravel()\n",
    "\n",
    "\n",
    "def get_oof(clf):\n",
    "    '''\n",
    "        via:\n",
    "        https://www.kaggle.com/eliotbarr/house-prices-\n",
    "        advanced-regression-techniques/stacking-starter/code\n",
    "    '''\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((NFOLDS, ntest))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X_train)):\n",
    "        x_tr = X_train.iloc[train_index]\n",
    "        y_tr = y_train.iloc[train_index]\n",
    "        x_te = X_train.iloc[test_index]\n",
    "\n",
    "        clf.train(x_tr, y_tr)\n",
    "\n",
    "        oof_train[test_index] = clf.predict(x_te)\n",
    "        oof_test_skf[i, :] = clf.predict(X_test)\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\n",
    "\n",
    "\n",
    "get_rmse = lambda x,y: np.sqrt(np.mean(((x.ravel()*max_y) - np.log(y.ravel()+1))**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set up training and test data\n",
    "cols = [c for c in processed.train.columns if c != 'SalePrice' ]\n",
    "X_train = processed.train.loc[:, cols ]\n",
    "y_train = processed.train['SalePrice']\n",
    "X_test = processed.test.loc[:, cols]\n",
    "\n",
    "# set up model arrays \n",
    "train_models = []\n",
    "test_models = []\n",
    "\n",
    "# seeds \n",
    "seeds = np.random.choice(range(100), 3, replace=False)\n",
    "\n",
    "# loop\n",
    "for s in seeds:\n",
    "    ## set up K Folds ##\n",
    "    NFOLDS = 3\n",
    "    kf = KFold(n_splits=NFOLDS, \n",
    "               shuffle=True, \n",
    "               random_state=s)\n",
    "\n",
    "    ## Categorical embedding NN models ##\n",
    "    emb_model = gen_emb_nn_model(seed=s)\n",
    "    NN = NnWrapper(emb_model, emb=True)\n",
    "    nn_oof_train, nn_oof_test = get_oof(NN)\n",
    "    train_models.append(nn_oof_train)\n",
    "    test_models.append(nn_oof_test)\n",
    "    logging.warn('NNE score, {}: {:,.4f}'.format(s, get_rmse(nn_oof_train, y_train)))\n",
    "\n",
    "    ## Vanilla NN models ##\n",
    "    vanilla_nn_model = gen_vanilla_nn(seed=s)\n",
    "    NN2 = NnWrapper(vanilla_nn_model, emb=False)\n",
    "    nn2_oof_train, nn2_oof_test = get_oof(NN2)\n",
    "    train_models.append(nn2_oof_train)\n",
    "    test_models.append(nn2_oof_test)\n",
    "    logging.warn('NNV score, {}: {:,.4f}'.format(s, get_rmse(nn2_oof_train, y_train)))\n",
    "\n",
    "    ## XGBoost ##\n",
    "    base_xgb_params ={\n",
    "        'colsample_bytree': 0.75 ,\n",
    "        'silent': 1 ,\n",
    "        'subsample': 0.5 ,\n",
    "        'learning_rate': 0.05 ,\n",
    "        'objective': 'reg:linear' ,\n",
    "        'max_depth': 4 ,\n",
    "        'num_parallel_tree': 1 ,\n",
    "        'min_child_weight': 1 ,\n",
    "        'eval_metric': 'rmse' ,\n",
    "        'nrounds': 700 ,\n",
    "    }\n",
    "    base_xgb_params['seed'] = 2\n",
    "    XG = XgbWrapper(params=base_xgb_params)\n",
    "    xgb_oof_train, xgb_oof_test = get_oof(XG)\n",
    "    train_models.append(xgb_oof_train)\n",
    "    test_models.append(xgb_oof_test)\n",
    "    logging.warn('XGB score, {}: {:,.4f}'.format(s, get_rmse(xgb_oof_train, y_train)))\n",
    "\n",
    "    ## other models ##\n",
    "    # build models\n",
    "    LS = SklearnWrapper(clf=linear_model.Lasso, params={'alpha':0.0001}, seed=s)\n",
    "    RG = SklearnWrapper(clf=linear_model.Ridge, params={'alpha':10.0}, seed=s)\n",
    "    RF = SklearnWrapper(clf=RandomForestRegressor, params={\n",
    "                                                    'n_jobs': 4,\n",
    "                                                    'n_estimators': 400,\n",
    "                                                    'max_features': 0.5,\n",
    "                                                    'max_depth': 12,\n",
    "                                                    'min_samples_leaf': 10,\n",
    "                                                }, seed=s)\n",
    "    ET = SklearnWrapper(clf=ExtraTreesRegressor, params={\n",
    "                                                    'n_jobs': 4,\n",
    "                                                    'n_estimators': 400,\n",
    "                                                    'max_features': 0.5,\n",
    "                                                    'max_depth': 12,\n",
    "                                                    'min_samples_leaf': 10,\n",
    "                                                }, seed=s)\n",
    "    SVRL = SklearnWrapper(clf=SVR, params={'kernel':'linear','C':0.0001,'epsilon':0.001},seed=s)\n",
    "    SVRB = SklearnWrapper(clf=SVR, params={'kernel':'rbf','C':1.0,'epsilon':0.001},seed=s)\n",
    "    VPW = SklearnWrapper(clf=VWRegressor, params={'l':10.0,'power_t':0.1},seed=s)\n",
    "\n",
    "    # run models\n",
    "    ls_oof_train, ls_oof_test = get_oof(LS)\n",
    "    rg_oof_train, rg_oof_test = get_oof(RG)\n",
    "    rf_oof_train, rf_oof_test = get_oof(RF)\n",
    "    et_oof_train, et_oof_test = get_oof(ET)\n",
    "    svrl_oof_train, svrl_oof_test = get_oof(SVRL)\n",
    "    svrb_oof_train, svrb_oof_test = get_oof(SVRB)\n",
    "    vpw_oof_train, vpw_oof_test = get_oof(VPW)\n",
    "\n",
    "    # append models\n",
    "    other_train_models = [\n",
    "        ls_oof_train,\n",
    "        rg_oof_train,\n",
    "        rf_oof_train,\n",
    "        et_oof_train,\n",
    "        svrl_oof_train,\n",
    "        svrb_oof_train,\n",
    "        vpw_oof_train,\n",
    "    ] \n",
    "    other_test_models = [\n",
    "        ls_oof_test,\n",
    "        rg_oof_test,\n",
    "        rf_oof_test,\n",
    "        et_oof_test,\n",
    "        svrl_oof_test,\n",
    "        svrb_oof_test,\n",
    "        vpw_oof_test,\n",
    "    ]\n",
    "    train_models += other_train_models\n",
    "    test_models += other_test_models\n",
    "    \n",
    "    # log scores\n",
    "    lookup = ['LS','RG','RF','ET','SVRL','SVRB','VPW']\n",
    "    for i, m in enumerate(other_train_models):\n",
    "        logging.warn('{} score, {}: {:,.4f}'.format(lookup[i], s, get_rmse(m, y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = np.concatenate(train_models, axis=1)\n",
    "X_test = np.concatenate(test_models, axis=1)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=preprocessing_Y(y_train))\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "xgb_params = {\n",
    "    'seed': 0,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'silent': 1,\n",
    "    'subsample': 0.6,\n",
    "    'learning_rate': 0.01,\n",
    "    'objective': 'reg:linear',\n",
    "    'max_depth': 1,\n",
    "    'num_parallel_tree': 1,\n",
    "    'min_child_weight': 1,\n",
    "    'eval_metric': 'rmse',\n",
    "}\n",
    "\n",
    "res = xgb.cv(xgb_params, \n",
    "             dtrain, \n",
    "             num_boost_round=1000, \n",
    "             nfold=4, \n",
    "             stratified=False,\n",
    "             early_stopping_rounds=25, \n",
    "             verbose_eval=200, \n",
    "             show_stdv=True)\n",
    "best_nrounds = res.shape[0] - 1\n",
    "xgb_final_model = xgb.train(xgb_params, dtrain, best_nrounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "calc_exp = lambda x: np.exp(x*max_y)-1\n",
    "print('Ensemble:\\t{:,.4f}+{:,.4f}'.format(calc_exp(res.iloc[-1,0]),calc_exp(res.iloc[-1,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = np.exp(xgb_final_model.predict(dtest)*max_y)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(np.vstack((raw.test.index.astype(str).ravel(), \n",
    "                                     predictions.ravel())).T,\n",
    "                          columns=['Id','SalePrice'])\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
